# MAIR-Hub

**MAIR-Hub** (MAIR stands for **M**ultimodal **AI** **R**esources.) is a central repository for **M**ultimodal **AI** **R**esources. This hub serves as a comprehensive collection of tutorials, code examples and other assets related to multimodal AI research and applications.

## Repository Structure

The following directories contain specialized resources for different aspects of multimodal AI:

| Directory | Description |
|-----------|-------------|
| [rl-tutorial](./rl-tutorial/) | Reinforcement Learning tutorials, including RL experiments with step-by-step guidance for reproduction |
| [external-resources](#external-resources) | Curated links to other valuable multimodal AI resources |

### RL-Tutorial

The RL-Tutorial directory contains resources focused on reinforcement learning approaches in multimodal AI:

- [r1-zero](./rl-tutorial/r1-zero/): Tutorial of using the veRL framework to reproduce the reinforcement learning training process of DeepSeek-R1-Zero in the mathematics domain.
- [r1-like](./rl-tutorial/r1-like/): Tutorial of using the openRLHF framework to reproduce the reinforcement learning training process of DeepSeek-R1 in the mathematics domain.

### External Resources

This section provides links to valuable external tutorials and resources related to multimodal AI:

#### Reasoning and Knowledge Distillation

- [Distilling DeepSeek R1 into Qwen](https://github.com/NVIDIA/NeMo/blob/main/tutorials/llm/distill_deepseek_r1/REAMDE.rst): A tutorial demonstrating how to distill the reasoning abilities of DeepSeek R1 (a 671B parameter MoE model) into smaller models like Qwen using the NVIDIA NeMo 2.0 Framework. The repository includes notebooks for extracting reasoning data and training models with the distilled knowledge.

We are working on adding more tutorials and assets...

This project is licensed under the terms of the LICENSE file included in the repository.