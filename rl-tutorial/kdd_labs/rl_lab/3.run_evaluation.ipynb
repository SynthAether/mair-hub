{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0f74f3d",
   "metadata": {},
   "source": [
    "# Model Evaluation: Testing GRPO-Trained Mathematical Reasoning Models\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook is designed to evaluate the performance of language models that have been trained using **Group Relative Policy Optimization (GRPO)** from the tutorial in `1. grpo_training_nemo_rl.ipynb`. After completing the GRPO training process, you can use this notebook to assess how well your model has learned to solve mathematical reasoning problems.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before running this evaluation:\n",
    "\n",
    "1. **Completed Training**: You should have successfully run the GRPO training from the first notebook\n",
    "2. **Model Checkpoints**: Training should have generated model checkpoints in `results/grpo/step_X/`\n",
    "3. **System Requirements**: Sufficient GPU memory for model inference during evaluation\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a0679c",
   "metadata": {},
   "source": [
    "## Convert Checkpoint to Huggingface Format\n",
    "We first need to convert the checkpoint to Huggingface format to begin our next steps.\n",
    "Replace STEP_X with the step number of the checkpoint you want to evaluate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92ef5406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved HF checkpoint to: results/grpo/hf\n"
     ]
    }
   ],
   "source": [
    "# Convert checkpoint to HuggingFace format (Replace STEP_X with the step number of the checkpoint you want to evaluate)\n",
    "\n",
    "!cd /root/verb-workspace/NeMo-RL && uv run python examples/convert_dcp_to_hf.py \\\n",
    "    --config results/grpo/step_130/config.yaml \\\n",
    "    --dcp-ckpt-path results/grpo/step_130/policy/weights/ \\\n",
    "    --hf-ckpt-path results/grpo/hf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e98bec7",
   "metadata": {},
   "source": [
    "## Run Eval for Model Trained with RL\n",
    "Now let's test our trained model on MATH500:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3a78a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded configuration from: /root/verb-workspace/NeMo-RL/examples/configs/eval.yaml\n",
      "Overrides: {'generation': {'model_name': '/root/verb-workspace/NeMo-RL/results/grpo/hf'}, 'data': {'dataset_name': 'HuggingFaceH4/MATH-500', 'dataset_key': 'test'}}\n",
      "Applied CLI overrides\n",
      "Final config:\n",
      "{'cluster': {'gpus_per_node': 1, 'num_nodes': 1},\n",
      " 'data': {'dataset_key': 'test',\n",
      "          'dataset_name': 'HuggingFaceH4/MATH-500',\n",
      "          'max_input_seq_length': 2048,\n",
      "          'problem_key': 'problem',\n",
      "          'prompt_file': None,\n",
      "          'solution_key': 'answer',\n",
      "          'system_prompt_file': None},\n",
      " 'env': {'math': {'num_workers': 8}},\n",
      " 'eval': {'metric': 'pass@1', 'num_tests_per_prompt': 1, 'seed': 42},\n",
      " 'generation': {'backend': 'vllm',\n",
      "                'max_new_tokens': 2048,\n",
      "                'model_name': '/root/verb-workspace/NeMo-RL/results/grpo/hf',\n",
      "                'num_prompts_per_step': -1,\n",
      "                'stop_strings': None,\n",
      "                'stop_token_ids': None,\n",
      "                'temperature': 0.0,\n",
      "                'top_k': -1,\n",
      "                'top_p': 1.0,\n",
      "                'vllm_cfg': {'gpu_memory_utilization': 0.9,\n",
      "                             'max_model_len': 2048,\n",
      "                             'precision': 'bfloat16',\n",
      "                             'tensor_parallel_size': 1}},\n",
      " 'tokenizer': {'chat_template': 'default',\n",
      "               'name': '/root/verb-workspace/NeMo-RL/results/grpo/hf'}}\n",
      "WARNING:root:UV_CACHE_DIR is not set, using default cache dir\n",
      "2025-07-21 08:31:13,570\tWARNING services.py:2072 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 8589594624 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=10.24gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2025-07-21 08:31:13,708\tINFO worker.py:1841 -- Started a local Ray instance.\n",
      "INFO:nemo_rl.distributed.virtual_cluster:Started local cluster with tag 'nrl_tag_ALL': {'node:__internal_head__': 1.0, 'nrl_tag_ALL': 1.0, 'node:172.17.0.2': 1.0, 'CPU': 26.0, 'object_store_memory': 10000000000.0, 'memory': 212164462592.0, 'GPU': 1.0, 'accelerator_type:H100': 1.0}\n",
      "Using tokenizer's default chat template\n",
      "\n",
      "▶ Setting up data...\n",
      "Map: 100%|██████████████████████████| 500/500 [00:00<00:00, 18227.70 examples/s]\n",
      "  ✓ Evaluation dataset loaded with 500 samples\n",
      "\n",
      "▶ Setting up compute cluster...\n",
      "  ✓ Ray cluster initialized with 1 nodes\n",
      "\n",
      "▶ Setting up model...\n",
      "INFO:nemo_rl.utils.venvs:NEMO_RL_VENV_DIR is set to /root/verb-workspace/NeMo-RL/venvs.\n",
      "Using CPython \u001b[36m3.12.11\u001b[39m\n",
      "Creating virtual environment at: \u001b[36mvenvs/nemo_rl.models.generation.vllm.VllmGenerationWorker\u001b[39m\n",
      "Activate with: \u001b[32msource venvs/nemo_rl.models.generation.vllm.VllmGenerationWorker/bin/activate\u001b[39m\n",
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`VIRTUAL_ENV=.venv` does not match the project environment path `venvs/nemo_rl.models.generation.vllm.VllmGenerationWorker` and will be ignored; use `--active` to target the active environment instead\u001b[0m\n",
      "Finished creating venv /root/verb-workspace/NeMo-RL/venvs/nemo_rl.models.generation.vllm.VllmGenerationWorker\n",
      "  ✓ Using vLLM backend for generation with /root/verb-workspace/NeMo-RL/results/grpo/hf\n",
      "\n",
      "============================================================\n",
      "                  SETUP COMPLETE\n",
      "============================================================\n",
      "\n",
      "\u001b[36m(VllmGenerationWorker pid=117543)\u001b[0m INFO 07-21 08:31:19 [__init__.py:239] Automatically detected platform cuda.\n",
      "\u001b[36m(VllmGenerationWorker pid=117543)\u001b[0m INFO 07-21 08:31:26 [config.py:689] This model supports multiple tasks: {'score', 'embed', 'reward', 'generate', 'classify'}. Defaulting to 'generate'.\n",
      "\u001b[36m(VllmGenerationWorker pid=117543)\u001b[0m INFO 07-21 08:31:26 [config.py:1901] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "\u001b[36m(VllmGenerationWorker pid=117543)\u001b[0m WARNING 07-21 08:31:26 [cuda.py:96] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "\u001b[36m(VllmGenerationWorker pid=117543)\u001b[0m INFO 07-21 08:31:27 [core.py:61] Initializing a V1 LLM engine (v0.8.4) with config: model='/root/verb-workspace/NeMo-RL/results/grpo/hf', speculative_config=None, tokenizer='/root/verb-workspace/NeMo-RL/results/grpo/hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/root/verb-workspace/NeMo-RL/results/grpo/hf, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}\n",
      "\u001b[36m(VllmGenerationWorker pid=117543)\u001b[0m INFO 07-21 08:31:27 [worker_base.py:589] Injected <class 'nemo_rl.models.generation.vllm_backend.VllmInternalWorkerExtension'> into <class 'vllm.v1.worker.gpu_worker.Worker'> for extended collective_rpc calls ['report_device_id', 'update_weights_from_ipc_handles']\n",
      "\u001b[36m(VllmGenerationWorker pid=117543)\u001b[0m WARNING 07-21 08:31:27 [utils.py:2444] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x71e918b648c0>\n",
      "\u001b[36m(VllmGenerationWorker pid=117543)\u001b[0m INFO 07-21 08:31:27 [parallel_state.py:959] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "\u001b[36m(VllmGenerationWorker pid=117543)\u001b[0m INFO 07-21 08:31:27 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "\u001b[36m(VllmGenerationWorker pid=117543)\u001b[0m INFO 07-21 08:31:27 [gpu_model_runner.py:1276] Starting to load model /root/verb-workspace/NeMo-RL/results/grpo/hf...\n",
      "\u001b[36m(VllmGenerationWorker pid=117543)\u001b[0m WARNING 07-21 08:31:28 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "Loading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[36m(VllmGenerationWorker pid=117543)\u001b[0m INFO 07-21 08:31:33 [loader.py:458] Loading weights took 5.51 seconds\n",
      "Loading pt checkpoint shards: 100% Completed | 1/1 [00:05<00:00,  5.48s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 1/1 [00:05<00:00,  5.48s/it]\n",
      "\u001b[36m(VllmGenerationWorker pid=117543)\u001b[0m \n",
      "\u001b[36m(VllmGenerationWorker pid=117543)\u001b[0m INFO 07-21 08:31:33 [gpu_model_runner.py:1291] Model loading took 2.9105 GiB and 5.601956 seconds\n",
      "\u001b[36m(VllmGenerationWorker pid=117543)\u001b[0m INFO 07-21 08:31:34 [kv_cache_utils.py:634] GPU KV cache size: 2,320,656 tokens\n",
      "\u001b[36m(VllmGenerationWorker pid=117543)\u001b[0m INFO 07-21 08:31:34 [kv_cache_utils.py:637] Maximum concurrency for 2,048 tokens per request: 1133.13x\n",
      "\u001b[36m(VllmGenerationWorker pid=117543)\u001b[0m INFO 07-21 08:31:34 [core.py:163] init engine (profile, create kv cache, warmup model) took 0.42 seconds\n",
      "Processed prompts:   0%|          | 0/500 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts:   0%|          | 1/500 [00:00<03:57,  2.10it/s, est. speed input: 753.34 toks/s, output: 14.73 toks/s]\n",
      "Processed prompts:   1%|          | 4/500 [00:00<01:03,  7.87it/s, est. speed input: 1644.91 toks/s, output: 65.27 toks/s]\n",
      "Processed prompts:   2%|▏         | 11/500 [00:00<00:23, 20.52it/s, est. speed input: 2918.77 toks/s, output: 254.32 toks/s]\n",
      "Processed prompts:   3%|▎         | 15/500 [00:01<00:39, 12.41it/s, est. speed input: 1854.24 toks/s, output: 307.74 toks/s]\n",
      "Processed prompts:   4%|▍         | 19/500 [00:01<00:37, 12.76it/s, est. speed input: 1607.48 toks/s, output: 436.91 toks/s]\n",
      "Processed prompts:   4%|▍         | 21/500 [00:01<00:38, 12.37it/s, est. speed input: 1489.36 toks/s, output: 504.19 toks/s]\n",
      "Processed prompts:   5%|▍         | 24/500 [00:01<00:35, 13.60it/s, est. speed input: 1417.81 toks/s, output: 621.29 toks/s]\n",
      "Processed prompts:   6%|▌         | 28/500 [00:02<00:26, 17.50it/s, est. speed input: 1586.44 toks/s, output: 814.45 toks/s]\n",
      "Processed prompts:   7%|▋         | 35/500 [00:02<00:17, 26.53it/s, est. speed input: 1761.98 toks/s, output: 1171.71 toks/s]\n",
      "Processed prompts:   8%|▊         | 39/500 [00:02<00:17, 26.32it/s, est. speed input: 1784.24 toks/s, output: 1322.42 toks/s]\n",
      "Processed prompts:   9%|▉         | 46/500 [00:02<00:13, 34.40it/s, est. speed input: 1887.27 toks/s, output: 1671.71 toks/s]\n",
      "Processed prompts:  10%|█         | 51/500 [00:02<00:12, 36.59it/s, est. speed input: 1915.27 toks/s, output: 1894.05 toks/s]\n",
      "Processed prompts:  12%|█▏        | 58/500 [00:02<00:09, 44.25it/s, est. speed input: 1982.13 toks/s, output: 2240.69 toks/s]\n",
      "Processed prompts:  13%|█▎        | 63/500 [00:02<00:10, 42.80it/s, est. speed input: 1976.88 toks/s, output: 2440.11 toks/s]\n",
      "Processed prompts:  14%|█▎        | 68/500 [00:02<00:10, 40.35it/s, est. speed input: 2038.70 toks/s, output: 2623.66 toks/s]\n",
      "Processed prompts:  15%|█▍        | 74/500 [00:03<00:10, 42.31it/s, est. speed input: 2090.55 toks/s, output: 2880.52 toks/s]\n",
      "Processed prompts:  16%|█▋        | 82/500 [00:03<00:08, 50.00it/s, est. speed input: 2201.49 toks/s, output: 3264.55 toks/s]\n",
      "Processed prompts:  18%|█▊        | 89/500 [00:03<00:07, 51.50it/s, est. speed input: 2300.55 toks/s, output: 3569.18 toks/s]\n",
      "Processed prompts:  19%|█▉        | 95/500 [00:03<00:08, 48.67it/s, est. speed input: 2334.43 toks/s, output: 3794.78 toks/s]\n",
      "Processed prompts:  21%|██        | 104/500 [00:03<00:06, 57.18it/s, est. speed input: 2417.53 toks/s, output: 4236.53 toks/s]\n",
      "Processed prompts:  23%|██▎       | 114/500 [00:03<00:06, 63.56it/s, est. speed input: 2604.46 toks/s, output: 4714.46 toks/s]\n",
      "Processed prompts:  24%|██▍       | 122/500 [00:03<00:05, 65.57it/s, est. speed input: 2702.54 toks/s, output: 5075.94 toks/s]\n",
      "Processed prompts:  26%|██▌       | 130/500 [00:03<00:05, 67.55it/s, est. speed input: 2784.92 toks/s, output: 5437.93 toks/s]\n",
      "Processed prompts:  27%|██▋       | 137/500 [00:04<00:05, 66.40it/s, est. speed input: 2882.42 toks/s, output: 5730.07 toks/s]\n",
      "Processed prompts:  29%|██▉       | 145/500 [00:04<00:05, 68.12it/s, est. speed input: 3052.28 toks/s, output: 6087.51 toks/s]\n",
      "Processed prompts:  31%|███       | 155/500 [00:04<00:04, 74.47it/s, est. speed input: 3121.81 toks/s, output: 6566.88 toks/s]\n",
      "Processed prompts:  33%|███▎      | 163/500 [00:04<00:05, 61.12it/s, est. speed input: 3130.39 toks/s, output: 6795.10 toks/s]\n",
      "Processed prompts:  34%|███▍      | 170/500 [00:04<00:06, 53.69it/s, est. speed input: 3145.94 toks/s, output: 6988.67 toks/s]\n",
      "Processed prompts:  36%|███▌      | 179/500 [00:04<00:05, 60.65it/s, est. speed input: 3214.62 toks/s, output: 7410.86 toks/s]\n",
      "Processed prompts:  37%|███▋      | 187/500 [00:04<00:04, 63.67it/s, est. speed input: 3286.74 toks/s, output: 7760.41 toks/s]\n",
      "Processed prompts:  39%|███▉      | 195/500 [00:05<00:05, 52.73it/s, est. speed input: 3252.92 toks/s, output: 7941.68 toks/s]\n",
      "Processed prompts:  40%|████      | 201/500 [00:05<00:05, 50.66it/s, est. speed input: 3390.49 toks/s, output: 8129.62 toks/s]\n",
      "Processed prompts:  41%|████▏     | 207/500 [00:05<00:05, 49.03it/s, est. speed input: 3399.05 toks/s, output: 8320.14 toks/s]\n",
      "Processed prompts:  43%|████▎     | 216/500 [00:05<00:05, 55.99it/s, est. speed input: 3463.85 toks/s, output: 8730.01 toks/s]\n",
      "Processed prompts:  44%|████▍     | 222/500 [00:05<00:04, 56.31it/s, est. speed input: 3480.76 toks/s, output: 8962.42 toks/s]\n",
      "Processed prompts:  46%|████▌     | 228/500 [00:05<00:04, 56.55it/s, est. speed input: 3499.65 toks/s, output: 9195.10 toks/s]\n",
      "Processed prompts:  47%|████▋     | 235/500 [00:05<00:04, 59.27it/s, est. speed input: 3528.10 toks/s, output: 9493.61 toks/s]\n",
      "Processed prompts:  48%|████▊     | 242/500 [00:05<00:05, 50.95it/s, est. speed input: 3524.14 toks/s, output: 9667.29 toks/s]\n",
      "Processed prompts:  50%|████▉     | 248/500 [00:06<00:05, 47.24it/s, est. speed input: 3501.82 toks/s, output: 9828.24 toks/s]\n",
      "Processed prompts:  52%|█████▏    | 258/500 [00:06<00:04, 53.23it/s, est. speed input: 3531.49 toks/s, output: 10259.18 toks/s]\n",
      "Processed prompts:  53%|█████▎    | 267/500 [00:06<00:03, 60.11it/s, est. speed input: 3641.20 toks/s, output: 10687.81 toks/s]\n",
      "Processed prompts:  55%|█████▍    | 274/500 [00:06<00:03, 61.05it/s, est. speed input: 3648.34 toks/s, output: 10980.94 toks/s]\n",
      "Processed prompts:  56%|█████▌    | 281/500 [00:06<00:03, 56.91it/s, est. speed input: 3679.70 toks/s, output: 11217.00 toks/s]\n",
      "Processed prompts:  57%|█████▋    | 287/500 [00:06<00:03, 53.48it/s, est. speed input: 3687.51 toks/s, output: 11407.05 toks/s]\n",
      "Processed prompts:  59%|█████▊    | 293/500 [00:06<00:05, 39.39it/s, est. speed input: 3617.42 toks/s, output: 11386.83 toks/s]\n",
      "Processed prompts:  60%|█████▉    | 298/500 [00:07<00:05, 39.14it/s, est. speed input: 3615.72 toks/s, output: 11525.84 toks/s]\n",
      "Processed prompts:  61%|██████    | 304/500 [00:07<00:04, 42.00it/s, est. speed input: 3641.47 toks/s, output: 11755.52 toks/s]\n",
      "Processed prompts:  62%|██████▏   | 309/500 [00:07<00:04, 42.15it/s, est. speed input: 3632.37 toks/s, output: 11916.53 toks/s]\n",
      "Processed prompts:  63%|██████▎   | 315/500 [00:07<00:04, 45.76it/s, est. speed input: 3636.40 toks/s, output: 12169.55 toks/s]\n",
      "Processed prompts:  64%|██████▍   | 320/500 [00:07<00:04, 41.63it/s, est. speed input: 3634.73 toks/s, output: 12280.97 toks/s]\n",
      "Processed prompts:  65%|██████▌   | 325/500 [00:07<00:05, 32.59it/s, est. speed input: 3659.86 toks/s, output: 12255.89 toks/s]\n",
      "Processed prompts:  66%|██████▌   | 329/500 [00:08<00:09, 17.75it/s, est. speed input: 3449.14 toks/s, output: 11728.10 toks/s]\n",
      "Processed prompts:  67%|██████▋   | 333/500 [00:08<00:08, 18.97it/s, est. speed input: 3435.67 toks/s, output: 11785.58 toks/s]\n",
      "Processed prompts:  67%|██████▋   | 336/500 [00:09<00:12, 13.32it/s, est. speed input: 3291.96 toks/s, output: 11392.05 toks/s]\n",
      "Processed prompts:  68%|██████▊   | 339/500 [00:09<00:11, 13.44it/s, est. speed input: 3236.51 toks/s, output: 11346.23 toks/s]\n",
      "Processed prompts:  68%|██████▊   | 341/500 [00:09<00:13, 11.44it/s, est. speed input: 3163.65 toks/s, output: 11146.95 toks/s]\n",
      "Processed prompts:  69%|██████▉   | 345/500 [00:09<00:11, 13.91it/s, est. speed input: 3136.58 toks/s, output: 11248.52 toks/s]\n",
      "Processed prompts:  69%|██████▉   | 347/500 [00:10<00:20,  7.59it/s, est. speed input: 2925.32 toks/s, output: 10586.76 toks/s]\n",
      "Processed prompts:  70%|██████▉   | 349/500 [00:10<00:20,  7.30it/s, est. speed input: 2867.69 toks/s, output: 10431.79 toks/s]\n",
      "Processed prompts:  70%|███████   | 351/500 [00:10<00:18,  7.88it/s, est. speed input: 2842.13 toks/s, output: 10403.71 toks/s]\n",
      "Processed prompts:  71%|███████   | 353/500 [00:11<00:26,  5.56it/s, est. speed input: 2700.35 toks/s, output: 9952.69 toks/s] \n",
      "Processed prompts:  71%|███████   | 354/500 [00:12<00:38,  3.81it/s, est. speed input: 2551.39 toks/s, output: 9452.71 toks/s]\n",
      "Processed prompts:  71%|███████   | 355/500 [00:12<00:45,  3.18it/s, est. speed input: 2448.05 toks/s, output: 9126.39 toks/s]\n",
      "Processed prompts:  71%|███████   | 356/500 [00:12<00:38,  3.69it/s, est. speed input: 2435.37 toks/s, output: 9134.75 toks/s]\n",
      "Processed prompts:  71%|███████▏  | 357/500 [00:16<02:17,  1.04it/s, est. speed input: 1946.06 toks/s, output: 7356.23 toks/s]\n",
      "Processed prompts:  72%|███████▏  | 359/500 [00:19<02:39,  1.13s/it, est. speed input: 1701.63 toks/s, output: 6426.32 toks/s]\n",
      "Processed prompts:  72%|███████▏  | 360/500 [00:21<03:12,  1.37s/it, est. speed input: 1538.33 toks/s, output: 5839.99 toks/s]\n",
      "Processed prompts:  72%|███████▏  | 361/500 [00:21<02:36,  1.12s/it, est. speed input: 1526.76 toks/s, output: 5831.94 toks/s]\n",
      "Processed prompts:  73%|███████▎  | 363/500 [00:21<01:36,  1.42it/s, est. speed input: 1537.67 toks/s, output: 5947.48 toks/s]\n",
      "Processed prompts:  73%|███████▎  | 367/500 [00:21<00:45,  2.91it/s, est. speed input: 1573.26 toks/s, output: 6242.86 toks/s]\n",
      "Processed prompts:  74%|███████▍  | 369/500 [00:22<00:41,  3.15it/s, est. speed input: 1556.90 toks/s, output: 6267.85 toks/s]\n",
      "Processed prompts:  74%|███████▍  | 371/500 [00:22<00:33,  3.88it/s, est. speed input: 1558.74 toks/s, output: 6372.72 toks/s]\n",
      "Processed prompts:  75%|███████▍  | 373/500 [00:22<00:25,  4.98it/s, est. speed input: 1565.74 toks/s, output: 6503.53 toks/s]\n",
      "Processed prompts:  75%|███████▌  | 375/500 [00:23<00:22,  5.52it/s, est. speed input: 1560.29 toks/s, output: 6593.02 toks/s]\n",
      "Processed prompts:  76%|███████▌  | 379/500 [00:23<00:13,  8.91it/s, est. speed input: 1576.75 toks/s, output: 6888.52 toks/s]\n",
      "Processed prompts:  76%|███████▋  | 382/500 [00:23<00:10, 11.19it/s, est. speed input: 1584.91 toks/s, output: 7097.65 toks/s]\n",
      "Processed prompts:  77%|███████▋  | 386/500 [00:23<00:07, 15.38it/s, est. speed input: 1598.11 toks/s, output: 7395.70 toks/s]\n",
      "Processed prompts:  78%|███████▊  | 389/500 [00:23<00:06, 17.51it/s, est. speed input: 1604.38 toks/s, output: 7607.10 toks/s]\n",
      "Processed prompts:  79%|███████▉  | 396/500 [00:23<00:03, 27.03it/s, est. speed input: 1626.96 toks/s, output: 8146.36 toks/s]\n",
      "Processed prompts:  82%|████████▏ | 408/500 [00:23<00:02, 45.86it/s, est. speed input: 1665.52 toks/s, output: 9096.73 toks/s]\n",
      "Processed prompts:  84%|████████▎ | 418/500 [00:23<00:01, 57.11it/s, est. speed input: 1692.76 toks/s, output: 9878.38 toks/s]\n",
      "Processed prompts:  86%|████████▌ | 428/500 [00:23<00:01, 66.55it/s, est. speed input: 1715.22 toks/s, output: 10659.00 toks/s]\n",
      "Processed prompts:  91%|█████████ | 455/500 [00:24<00:00, 115.57it/s, est. speed input: 1776.95 toks/s, output: 12836.96 toks/s]\n",
      "Processed prompts:  96%|█████████▋| 482/500 [00:24<00:00, 152.60it/s, est. speed input: 1828.74 toks/s, output: 15005.31 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:24<00:00, 20.54it/s, est. speed input: 1847.77 toks/s, output: 16399.70 toks/s] \n",
      "\n",
      "============================================================\n",
      "model_name='hf' dataset_name='MATH-500'\n",
      "max_new_tokens=2048 temperature=0.0 top_p=1.0 top_k=-1\n",
      "\n",
      "metric='pass@1' num_tests_per_prompt=1\n",
      "\n",
      "score=0.3540 (177.0/500)\n",
      "============================================================\n",
      "\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Run evaluation for model trained with RL\n",
    "!cd /root/verb-workspace/NeMo-RL && uv run python examples/run_eval.py \\\n",
    "    generation.model_name=$PWD/results/grpo/hf \\\n",
    "    data.dataset_name=HuggingFaceH4/MATH-500 \\\n",
    "    data.dataset_key=test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a7de7d",
   "metadata": {},
   "source": [
    "## Run Eval for Base Model\n",
    "We then test the base model on MATH500 for a comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f2c4826-06df-4b77-bf1a-0ef981de50e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded configuration from: /root/verb-workspace/NeMo-RL/examples/configs/eval.yaml\n",
      "Overrides: {'generation': {'model_name': 'Qwen/Qwen2.5-1.5B'}, 'data': {'dataset_name': 'HuggingFaceH4/MATH-500', 'dataset_key': 'test'}}\n",
      "Applied CLI overrides\n",
      "Final config:\n",
      "{'cluster': {'gpus_per_node': 1, 'num_nodes': 1},\n",
      " 'data': {'dataset_key': 'test',\n",
      "          'dataset_name': 'HuggingFaceH4/MATH-500',\n",
      "          'max_input_seq_length': 2048,\n",
      "          'problem_key': 'problem',\n",
      "          'prompt_file': None,\n",
      "          'solution_key': 'answer',\n",
      "          'system_prompt_file': None},\n",
      " 'env': {'math': {'num_workers': 8}},\n",
      " 'eval': {'metric': 'pass@1', 'num_tests_per_prompt': 1, 'seed': 42},\n",
      " 'generation': {'backend': 'vllm',\n",
      "                'max_new_tokens': 2048,\n",
      "                'model_name': 'Qwen/Qwen2.5-1.5B',\n",
      "                'num_prompts_per_step': -1,\n",
      "                'stop_strings': None,\n",
      "                'stop_token_ids': None,\n",
      "                'temperature': 0.0,\n",
      "                'top_k': -1,\n",
      "                'top_p': 1.0,\n",
      "                'vllm_cfg': {'gpu_memory_utilization': 0.9,\n",
      "                             'max_model_len': 2048,\n",
      "                             'precision': 'bfloat16',\n",
      "                             'tensor_parallel_size': 1}},\n",
      " 'tokenizer': {'chat_template': 'default', 'name': 'Qwen/Qwen2.5-1.5B'}}\n",
      "WARNING:root:UV_CACHE_DIR is not set, using default cache dir\n",
      "2025-07-21 08:33:28,104\tWARNING services.py:2072 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 8589594624 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=10.24gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2025-07-21 08:33:28,243\tINFO worker.py:1841 -- Started a local Ray instance.\n",
      "INFO:nemo_rl.distributed.virtual_cluster:Started local cluster with tag 'nrl_tag_ALL': {'node:__internal_head__': 1.0, 'nrl_tag_ALL': 1.0, 'node:172.17.0.2': 1.0, 'CPU': 26.0, 'memory': 212162684928.0, 'object_store_memory': 10000000000.0, 'GPU': 1.0, 'accelerator_type:H100': 1.0}\n",
      "Using tokenizer's default chat template\n",
      "\n",
      "▶ Setting up data...\n",
      "  ✓ Evaluation dataset loaded with 500 samples\n",
      "\n",
      "▶ Setting up compute cluster...\n",
      "  ✓ Ray cluster initialized with 1 nodes\n",
      "\n",
      "▶ Setting up model...\n",
      "INFO:nemo_rl.utils.venvs:NEMO_RL_VENV_DIR is set to /root/verb-workspace/NeMo-RL/venvs.\n",
      "Using CPython \u001b[36m3.12.11\u001b[39m\n",
      "Creating virtual environment at: \u001b[36mvenvs/nemo_rl.models.generation.vllm.VllmGenerationWorker\u001b[39m\n",
      "Activate with: \u001b[32msource venvs/nemo_rl.models.generation.vllm.VllmGenerationWorker/bin/activate\u001b[39m\n",
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`VIRTUAL_ENV=.venv` does not match the project environment path `venvs/nemo_rl.models.generation.vllm.VllmGenerationWorker` and will be ignored; use `--active` to target the active environment instead\u001b[0m\n",
      "Finished creating venv /root/verb-workspace/NeMo-RL/venvs/nemo_rl.models.generation.vllm.VllmGenerationWorker\n",
      "  ✓ Using vLLM backend for generation with Qwen/Qwen2.5-1.5B\n",
      "\n",
      "============================================================\n",
      "                  SETUP COMPLETE\n",
      "============================================================\n",
      "\n",
      "\u001b[36m(VllmGenerationWorker pid=120945)\u001b[0m INFO 07-21 08:33:34 [__init__.py:239] Automatically detected platform cuda.\n",
      "\u001b[36m(VllmGenerationWorker pid=120945)\u001b[0m INFO 07-21 08:33:41 [config.py:689] This model supports multiple tasks: {'reward', 'score', 'generate', 'embed', 'classify'}. Defaulting to 'generate'.\n",
      "\u001b[36m(VllmGenerationWorker pid=120945)\u001b[0m INFO 07-21 08:33:41 [config.py:1901] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "\u001b[36m(VllmGenerationWorker pid=120945)\u001b[0m WARNING 07-21 08:33:41 [cuda.py:96] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "\u001b[36m(VllmGenerationWorker pid=120945)\u001b[0m INFO 07-21 08:33:42 [core.py:61] Initializing a V1 LLM engine (v0.8.4) with config: model='Qwen/Qwen2.5-1.5B', speculative_config=None, tokenizer='Qwen/Qwen2.5-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-1.5B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}\n",
      "\u001b[36m(VllmGenerationWorker pid=120945)\u001b[0m INFO 07-21 08:33:42 [worker_base.py:589] Injected <class 'nemo_rl.models.generation.vllm_backend.VllmInternalWorkerExtension'> into <class 'vllm.v1.worker.gpu_worker.Worker'> for extended collective_rpc calls ['report_device_id', 'update_weights_from_ipc_handles']\n",
      "\u001b[36m(VllmGenerationWorker pid=120945)\u001b[0m WARNING 07-21 08:33:42 [utils.py:2444] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7276a3454680>\n",
      "\u001b[36m(VllmGenerationWorker pid=120945)\u001b[0m INFO 07-21 08:33:42 [parallel_state.py:959] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "\u001b[36m(VllmGenerationWorker pid=120945)\u001b[0m INFO 07-21 08:33:42 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "\u001b[36m(VllmGenerationWorker pid=120945)\u001b[0m INFO 07-21 08:33:42 [gpu_model_runner.py:1276] Starting to load model Qwen/Qwen2.5-1.5B...\n",
      "\u001b[36m(VllmGenerationWorker pid=120945)\u001b[0m WARNING 07-21 08:33:43 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[36m(VllmGenerationWorker pid=120945)\u001b[0m INFO 07-21 08:33:43 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "\u001b[36m(VllmGenerationWorker pid=120945)\u001b[0m INFO 07-21 08:33:43 [weight_utils.py:315] No model.safetensors.index.json found in remote.\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[36m(VllmGenerationWorker pid=120945)\u001b[0m INFO 07-21 08:33:44 [loader.py:458] Loading weights took 0.51 seconds\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.13it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.13it/s]\n",
      "\u001b[36m(VllmGenerationWorker pid=120945)\u001b[0m \n",
      "\u001b[36m(VllmGenerationWorker pid=120945)\u001b[0m INFO 07-21 08:33:44 [gpu_model_runner.py:1291] Model loading took 2.9105 GiB and 1.301759 seconds\n",
      "\u001b[36m(VllmGenerationWorker pid=120945)\u001b[0m INFO 07-21 08:33:44 [kv_cache_utils.py:634] GPU KV cache size: 2,320,656 tokens\n",
      "\u001b[36m(VllmGenerationWorker pid=120945)\u001b[0m INFO 07-21 08:33:44 [kv_cache_utils.py:637] Maximum concurrency for 2,048 tokens per request: 1133.13x\n",
      "\u001b[36m(VllmGenerationWorker pid=120945)\u001b[0m INFO 07-21 08:33:45 [core.py:163] init engine (profile, create kv cache, warmup model) took 0.55 seconds\n",
      "Processed prompts:   0%|          | 0/500 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts:   0%|          | 1/500 [00:00<04:50,  1.72it/s, est. speed input: 54.89 toks/s, output: 24.01 toks/s]\n",
      "Processed prompts:   1%|          | 3/500 [00:00<01:50,  4.52it/s, est. speed input: 200.64 toks/s, output: 81.55 toks/s]\n",
      "Processed prompts:   1%|          | 4/500 [00:01<02:00,  4.11it/s, est. speed input: 207.22 toks/s, output: 107.87 toks/s]\n",
      "Processed prompts:   1%|          | 5/500 [00:01<02:40,  3.09it/s, est. speed input: 209.17 toks/s, output: 129.51 toks/s]\n",
      "Processed prompts:   2%|▏         | 9/500 [00:01<01:07,  7.29it/s, est. speed input: 412.36 toks/s, output: 326.94 toks/s]\n",
      "Processed prompts:   2%|▏         | 11/500 [00:02<02:13,  3.65it/s, est. speed input: 366.08 toks/s, output: 302.12 toks/s]\n",
      "Processed prompts:   2%|▏         | 12/500 [00:03<02:21,  3.46it/s, est. speed input: 336.86 toks/s, output: 331.21 toks/s]\n",
      "Processed prompts:   3%|▎         | 15/500 [00:03<01:25,  5.66it/s, est. speed input: 459.27 toks/s, output: 507.87 toks/s]\n",
      "Processed prompts:   3%|▎         | 17/500 [00:03<01:26,  5.61it/s, est. speed input: 453.01 toks/s, output: 579.39 toks/s]\n",
      "Processed prompts:   4%|▍         | 19/500 [00:04<01:58,  4.06it/s, est. speed input: 394.53 toks/s, output: 602.20 toks/s]\n",
      "Processed prompts:   4%|▍         | 21/500 [00:04<01:43,  4.61it/s, est. speed input: 386.81 toks/s, output: 691.77 toks/s]\n",
      "Processed prompts:   4%|▍         | 22/500 [00:06<04:06,  1.94it/s, est. speed input: 287.42 toks/s, output: 559.11 toks/s]\n",
      "Processed prompts:   5%|▍         | 23/500 [00:07<04:09,  1.91it/s, est. speed input: 284.35 toks/s, output: 581.55 toks/s]\n",
      "Processed prompts:   5%|▍         | 24/500 [00:11<11:01,  1.39s/it, est. speed input: 184.68 toks/s, output: 425.00 toks/s]\n",
      "Processed prompts:   5%|▌         | 25/500 [00:21<26:54,  3.40s/it, est. speed input: 138.60 toks/s, output: 289.66 toks/s]\n",
      "Processed prompts:   5%|▌         | 26/500 [00:23<24:03,  3.05s/it, est. speed input: 157.38 toks/s, output: 321.47 toks/s]\n",
      "Processed prompts:   5%|▌         | 27/500 [00:24<19:07,  2.43s/it, est. speed input: 181.02 toks/s, output: 368.14 toks/s]\n",
      "Processed prompts:   6%|▌         | 28/500 [00:29<24:44,  3.15s/it, est. speed input: 166.26 toks/s, output: 358.53 toks/s]\n",
      "Processed prompts:   6%|▌         | 29/500 [00:32<24:12,  3.08s/it, est. speed input: 162.52 toks/s, output: 378.19 toks/s]\n",
      "Processed prompts:   6%|▌         | 30/500 [00:32<17:37,  2.25s/it, est. speed input: 172.75 toks/s, output: 428.27 toks/s]\n",
      "Processed prompts:   6%|▋         | 32/500 [00:32<10:51,  1.39s/it, est. speed input: 190.08 toks/s, output: 522.46 toks/s]\n",
      "Processed prompts:   7%|▋         | 33/500 [00:34<10:13,  1.31s/it, est. speed input: 192.83 toks/s, output: 557.47 toks/s]\n",
      "Processed prompts:   7%|▋         | 34/500 [00:34<08:58,  1.15s/it, est. speed input: 196.71 toks/s, output: 597.43 toks/s]\n",
      "Processed prompts:   7%|▋         | 36/500 [00:34<05:24,  1.43it/s, est. speed input: 211.04 toks/s, output: 696.64 toks/s]\n",
      "Processed prompts:   8%|▊         | 38/500 [00:35<03:44,  2.06it/s, est. speed input: 224.26 toks/s, output: 793.17 toks/s]\n",
      "Processed prompts:   8%|▊         | 39/500 [00:35<03:07,  2.46it/s, est. speed input: 230.72 toks/s, output: 841.54 toks/s]\n",
      "Processed prompts:   8%|▊         | 41/500 [00:35<02:10,  3.52it/s, est. speed input: 243.65 toks/s, output: 939.12 toks/s]\n",
      "Processed prompts:   9%|▉         | 45/500 [00:35<01:12,  6.29it/s, est. speed input: 269.75 toks/s, output: 1137.37 toks/s]\n",
      "Processed prompts:   9%|▉         | 47/500 [00:36<01:19,  5.70it/s, est. speed input: 279.02 toks/s, output: 1224.52 toks/s]\n",
      "Processed prompts:  10%|█         | 50/500 [00:36<00:57,  7.81it/s, est. speed input: 296.24 toks/s, output: 1371.30 toks/s]\n",
      "Processed prompts:  10%|█         | 52/500 [00:36<00:50,  8.87it/s, est. speed input: 306.98 toks/s, output: 1466.98 toks/s]\n",
      "Processed prompts:  11%|█         | 54/500 [00:36<00:53,  8.37it/s, est. speed input: 316.01 toks/s, output: 1556.52 toks/s]\n",
      "Processed prompts:  11%|█         | 56/500 [00:36<00:48,  9.16it/s, est. speed input: 325.34 toks/s, output: 1650.20 toks/s]\n",
      "Processed prompts:  12%|█▏        | 59/500 [00:36<00:36, 12.19it/s, est. speed input: 340.26 toks/s, output: 1796.01 toks/s]\n",
      "Processed prompts:  12%|█▏        | 62/500 [00:37<00:42, 10.29it/s, est. speed input: 352.06 toks/s, output: 1927.51 toks/s]\n",
      "Processed prompts:  13%|█▎        | 64/500 [00:37<00:37, 11.55it/s, est. speed input: 360.51 toks/s, output: 2022.03 toks/s]\n",
      "Processed prompts:  13%|█▎        | 67/500 [00:37<00:32, 13.24it/s, est. speed input: 372.68 toks/s, output: 2163.13 toks/s]\n",
      "Processed prompts:  14%|█▍        | 70/500 [00:37<00:29, 14.62it/s, est. speed input: 384.29 toks/s, output: 2303.62 toks/s]\n",
      "Processed prompts:  15%|█▍        | 73/500 [00:37<00:29, 14.42it/s, est. speed input: 394.70 toks/s, output: 2440.13 toks/s]\n",
      "Processed prompts:  15%|█▌        | 75/500 [00:38<00:34, 12.28it/s, est. speed input: 400.04 toks/s, output: 2524.20 toks/s]\n",
      "Processed prompts:  16%|█▌        | 78/500 [00:38<00:27, 15.12it/s, est. speed input: 410.26 toks/s, output: 2666.40 toks/s]\n",
      "Processed prompts:  16%|█▌        | 80/500 [00:38<00:29, 14.37it/s, est. speed input: 415.75 toks/s, output: 2754.64 toks/s]\n",
      "Processed prompts:  17%|█▋        | 84/500 [00:38<00:21, 19.16it/s, est. speed input: 428.56 toks/s, output: 2945.61 toks/s]\n",
      "Processed prompts:  18%|█▊        | 88/500 [00:38<00:18, 22.06it/s, est. speed input: 440.49 toks/s, output: 3133.88 toks/s]\n",
      "Processed prompts:  19%|█▉        | 96/500 [00:38<00:12, 32.65it/s, est. speed input: 464.89 toks/s, output: 3519.58 toks/s]\n",
      "Processed prompts:  21%|██        | 103/500 [00:38<00:10, 38.42it/s, est. speed input: 485.08 toks/s, output: 3854.21 toks/s]\n",
      "Processed prompts:  22%|██▏       | 109/500 [00:39<00:09, 40.68it/s, est. speed input: 501.19 toks/s, output: 4138.39 toks/s]\n",
      "Processed prompts:  23%|██▎       | 116/500 [00:39<00:08, 45.11it/s, est. speed input: 519.31 toks/s, output: 4471.54 toks/s]\n",
      "Processed prompts:  25%|██▍       | 124/500 [00:39<00:07, 50.70it/s, est. speed input: 539.10 toks/s, output: 4852.97 toks/s]\n",
      "Processed prompts:  28%|██▊       | 140/500 [00:39<00:04, 73.51it/s, est. speed input: 577.80 toks/s, output: 5628.59 toks/s]\n",
      "Processed prompts:  31%|███       | 154/500 [00:39<00:04, 85.58it/s, est. speed input: 609.81 toks/s, output: 6302.60 toks/s]\n",
      "Processed prompts:  35%|███▍      | 173/500 [00:39<00:03, 107.19it/s, est. speed input: 651.39 toks/s, output: 7221.57 toks/s]\n",
      "Processed prompts:  38%|███▊      | 191/500 [00:39<00:02, 122.15it/s, est. speed input: 688.17 toks/s, output: 8089.46 toks/s]\n",
      "Processed prompts:  43%|████▎     | 216/500 [00:39<00:01, 153.04it/s, est. speed input: 737.18 toks/s, output: 9300.80 toks/s]\n",
      "Processed prompts:  48%|████▊     | 238/500 [00:39<00:01, 169.35it/s, est. speed input: 776.53 toks/s, output: 10362.23 toks/s]\n",
      "Processed prompts:  56%|█████▌    | 278/500 [00:40<00:00, 227.16it/s, est. speed input: 844.33 toks/s, output: 12307.30 toks/s]\n",
      "Processed prompts:  62%|██████▏   | 311/500 [00:40<00:00, 246.23it/s, est. speed input: 893.92 toks/s, output: 13901.35 toks/s]\n",
      "Processed prompts:  73%|███████▎  | 366/500 [00:40<00:00, 323.83it/s, est. speed input: 968.81 toks/s, output: 16581.20 toks/s]\n",
      "Processed prompts:  84%|████████▍ | 422/500 [00:40<00:00, 378.07it/s, est. speed input: 1035.69 toks/s, output: 19303.13 toks/s]\n",
      "Processed prompts:  97%|█████████▋| 483/500 [00:40<00:00, 434.72it/s, est. speed input: 1095.60 toks/s, output: 22271.91 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:40<00:00, 12.31it/s, est. speed input: 1107.04 toks/s, output: 23065.98 toks/s] \n",
      "\n",
      "============================================================\n",
      "model_name='Qwen2.5-1.5B' dataset_name='MATH-500'\n",
      "max_new_tokens=2048 temperature=0.0 top_p=1.0 top_k=-1\n",
      "\n",
      "metric='pass@1' num_tests_per_prompt=1\n",
      "\n",
      "score=0.0500 (25.0/500)\n",
      "============================================================\n",
      "\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Run evaluation for pre-RL base model\n",
    "!cd /root/verb-workspace/NeMo-RL && uv run python examples/run_eval.py \\\n",
    "    generation.model_name=Qwen/Qwen2.5-1.5B \\\n",
    "    data.dataset_name=HuggingFaceH4/MATH-500 \\\n",
    "    data.dataset_key=test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712e2493",
   "metadata": {},
   "source": [
    "## Results\n",
    "Your results for the model trained with RL should have a significantly higher score than the score for the base model, indicating that reinforcement learning fine-tuning has substantially enhanced the model's mathematical reasoning capabilities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
