{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distill Reasoning Data from DeepSeek-R1\n",
    "\n",
    "In the field of LLMs, reasoning models leverage deep thinking capabilities to significantly enhance model performance across complex scenarios. According to the [DeepSeek-R1](https://arxiv.org/abs/2501.12948) paper, the reasoning pattern of larger models can be distilled into smaller models. Specifically, we can distill long-chain-of-thought (long-CoT) data that includes reasoning processes from DeepSeek-R1 and directly fine-tune open-source models like Qwen and Llama. This straightforward distillation method significantly enhances the reasoning abilities of smaller models.\n",
    "\n",
    "\n",
    "To demonstrate the complete distillation process, we have prepared three notebooks that cover how to distill reasoning data from DeepSeek-R1 using the NIM API, how to train models using the distilled data, and how to evaluate the model.\n",
    "\n",
    "\n",
    "- [1.generate_reasoning_data.ipynb](./1.generate_reasoning_data.ipynb) (⭐) demonstrates how to distill reasoning data from DeepSeek-R1 using the NIM API. \n",
    "- [2.qwen2_distill_nemo.ipynb](./2.qwen2_distill_nemo.ipynb) shows how to train open-source models using the distilled data.\n",
    "- [3.evaluation.ipynb](./3.evaluation.ipynb) shows how the evaluate the model.\n",
    "\n",
    "\n",
    "This tutorial is part 1 of the series, and it will demonstrate how to distill reasoning data from the DeepSeek-R1 model using NVIDIA NIM.\n",
    "\n",
    "Prerequisites:\n",
    "- Obtain an NVIDIA API Key (visit [build.nvidia.com](https://build.nvidia.com/explore/discover) for details)\n",
    "\n",
    "This notebook contains three steps:\n",
    "1. Prepare the raw dataset\n",
    "2. Distill reasoning data from DeepSeek-R1 using NVIDIA NIM API\n",
    "3. Post-process the distilled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai math_verify datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env NVIDIA_API_KEY=nvapi-xxxxxxxxxx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Prepare Dataset\n",
    "\n",
    "During the training process of DeepSeek-R1-Zero, DeepSeek mentioned they used data from math, code, science, and logic domains. However, since they haven't disclosed the specific data sources, we will use open-source datasets as examples.\n",
    "\n",
    "In the following code, we will use the [open-r1/OpenR1-Math-220k](https://huggingface.co/datasets/open-r1/OpenR1-Math-220k) from HuggingFace. \n",
    "\n",
    "You can also create your own dataset, but it's best to align with the example dataset's format, ensuring each entry contains both a `question` and an `answer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating train split: 100%|██████████| 93733/93733 [00:15<00:00, 6142.21 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 93733\n",
      "===== Problem 1 =====\n",
      "## Task B-1.3.\n",
      "\n",
      "A ship traveling along a river has covered $24 \\mathrm{~km}$ upstream and $28 \\mathrm{~km}$ downstream. For this journey, it took half an hour less than for traveling $30 \\mathrm{~km}$ upstream and $21 \\mathrm{~km}$ downstream, or half an hour more than for traveling $15 \\mathrm{~km}$ upstream and $42 \\mathrm{~km}$ downstream, assuming that both the ship and the river move uniformly.\n",
      "\n",
      "Determine the speed of the ship in still water and the speed of the river.\n",
      "===== Answer 1 =====\n",
      "v_{R}=4\\mathrm{~}/\\mathrm{},v_{B}=10\\mathrm{~}/\\mathrm{}\n",
      "\n",
      "\n",
      "===== Problem 2 =====\n",
      "3. (6 points) A construction company was building a tunnel. When $\\frac{1}{3}$ of the tunnel was completed at the original speed, they started using new equipment, which increased the construction speed by $20 \\%$ and reduced the working hours to $80 \\%$ of the original. As a result, it took a total of 185 days to complete the tunnel. If they had not used the new equipment and continued at the original speed, it would have taken $\\qquad$ days to complete the tunnel.\n",
      "===== Answer 2 =====\n",
      "180\n",
      "\n",
      "\n",
      "===== Problem 3 =====\n",
      "Prove that number $1$ can be represented as a sum of a finite number $n$ of real numbers, less than $1,$ not necessarily  distinct, which contain in their decimal representation only the digits $0$ and/or $7.$ Which is the least possible number $n$?\n",
      "===== Answer 3 =====\n",
      " 8 \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"open-r1/OpenR1-Math-220k\", split=\"train\")\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "\n",
    "# Print the first few examples\n",
    "for i, example in enumerate(dataset.select(range(3))):\n",
    "    print(f\"===== Problem {i+1} =====\")\n",
    "    print(example[\"problem\"])\n",
    "    print(f\"===== Answer {i+1} =====\")\n",
    "    print(example[\"answer\"])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Distill Reasoning Data from DeepSeek-R1 Using NVIDIA NIM API\n",
    "\n",
    "DeepSeek recommends adhering to the following configurations when running inference the DeepSeek-R1 series of models, including benchmarking, to achieve the expected performance:\n",
    "\n",
    "- Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.\n",
    "- Avoid adding a system prompt; all instructions should be contained within the user prompt.\n",
    "- For mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell configures the NIM client and runs a basic distillation test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "  base_url = \"https://integrate.api.nvidia.com/v1\",\n",
    "  api_key = os.getenv(\"NVIDIA_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to figure out which number is larger between 9.11 and 9.8. Let me start by writing both numbers down to see them clearly: 9.11 and 9.8. Hmm, both are decimals, but they have a different number of digits after the decimal point. Maybe I should compare them place by place.\n",
      "\n",
      "First, let me look at the whole number part. Both numbers have 9 as the whole number, so that part is equal. Now, moving on to the decimal parts. For 9.11, the first decimal place is 1, and for 9.8, the first decimal place is 8. Wait, but 9.8 is the same as 9.80, right? Because adding a zero at the end of a decimal doesn't change its value. So if I rewrite 9.8 as 9.80, then both numbers have two decimal places, which might make it easier to compare.\n",
      "\n",
      "So now we have 9.11 and 9.80. Comparing the tenths place: 1 versus 8. Since 1 is less than 8, does that mean 9.11 is less than 9.80? Let me check that again. The tenths place is the first digit after the decimal, which represents tenths. So 0.1 versus 0.8. Yes, 0.1 is definitely less than 0.8. Then, even if the hundredths place in 9.11 is 1, it's only 0.01, whereas in 9.80, the hundredths place is 0. So adding those up, 9.11 is 9 + 0.1 + 0.01 = 9.11, and 9.80 is 9 + 0.8 + 0.00 = 9.80. \n",
      "\n",
      "But maybe another way to think about it is to convert both numbers to fractions or to the same denominator. Let's try converting them to fractions. \n",
      "\n",
      "9.11 can be written as 9 + 11/100, which is 9 11/100. Similarly, 9.8 is 9 + 8/10, which is 9 8/10. To compare these fractions, I need a common denominator. The denominators here are 100 and 10. The least common denominator is 100. So converting 8/10 to hundredths: 8/10 = 80/100. So 9.8 is 9 80/100. Comparing 9 11/100 and 9 80/100, clearly 80/100 is larger than 11/100. Therefore, 9.8 is larger than 9.11.\n",
      "\n",
      "Alternatively, if I subtract the two numbers to see which is bigger: 9.8 minus 9.11. Let's do that subtraction. \n",
      "\n",
      "9.80\n",
      "-9.11\n",
      "------\n",
      "0.69\n",
      "\n",
      "Since the result is positive, 9.8 is larger than 9.11 by 0.69. \n",
      "\n",
      "Wait, another way to verify is to think in terms of money. If these were amounts of money, $9.11 versus $9.80. Which is more? $9.80 is 9 dollars and 80 cents, while $9.11 is 9 dollars and 11 cents. Definitely, 80 cents is more than 11 cents, so $9.80 is more. \n",
      "\n",
      "Or maybe using a number line. If I imagine a number line with 9.0, 9.1, 9.2 up to 10.0. 9.11 is just a little past 9.1, while 9.8 is much further along, close to 10.0. So 9.8 is definitely to the right of 9.11 on the number line, which means it's larger.\n",
      "\n",
      "Is there any chance I might have mixed up the decimal places? For example, maybe confusing tenths and hundredths? Let me double-check. The first decimal place is tenths (1/10), the second is hundredths (1/100). So in 9.11, the first 1 is tenths (0.1) and the second 1 is hundredths (0.01). In 9.8, which is 9.80, the 8 is in the tenths place (0.8) and the 0 is in the hundredths place (0.00). So comparing tenths: 0.1 vs 0.8. Since 0.8 is larger, 9.8 is larger regardless of the hundredths place. Because even if the tenths place was equal, we would then look at the hundredths, but here the tenths already determine the larger number.\n",
      "\n",
      "Another angle: if I convert both numbers to percents. 9.11 as a percent would be 911%, and 9.8 as a percent would be 980%. Comparing 911% and 980%, clearly 980% is larger. So that confirms it again.\n",
      "\n",
      "Alternatively, multiplying both numbers by 100 to eliminate decimals. 9.11 * 100 = 911, and 9.8 * 100 = 980. Then compare 911 and 980. 980 is larger, so 9.8 is larger.\n",
      "\n",
      "Wait, but why did I even think there might be confusion? Maybe because 9.11 has two digits after the decimal and 9.8 only has one. But when we write 9.8 as 9.80, it's clear. Maybe some people might think more digits after decimal means a larger number, but that's not true. For example, 0.9 is larger than 0.8999 because the tenths place is higher. So even though 0.8999 has more digits, it's still smaller. So in this case, same thing. 9.8 is 9.80, which is larger than 9.11.\n",
      "\n",
      "So all methods point to 9.8 being larger than 9.11. Therefore, the answer should be 9.8 is the larger number.\n",
      "\n",
      "**Final Answer**\n",
      "The larger number is \\boxed{9.8}.\n",
      "</think>\n",
      "\n",
      "To determine which number is larger between 9.11 and 9.8, we start by comparing their whole number parts, which are both 9. Next, we compare the decimal parts. \n",
      "\n",
      "Rewriting 9.8 as 9.80 to have the same number of decimal places as 9.11, we get:\n",
      "- 9.11\n",
      "- 9.80\n",
      "\n",
      "Comparing the tenths place (the first decimal digit):\n",
      "- 9.11 has 1 in the tenths place.\n",
      "- 9.80 has 8 in the tenths place.\n",
      "\n",
      "Since 8 is greater than 1, 9.80 is larger than 9.11. This can be confirmed by converting both numbers to fractions with a common denominator, subtracting them, or using other methods like comparing them as money or on a number line. All methods consistently show that 9.8 is larger than 9.11.\n",
      "\n",
      "\\[\n",
      "\\boxed{9.8}\n",
      "\\]"
     ]
    }
   ],
   "source": [
    "# A simple test case\n",
    "problem = \"which number is larger, 9.11 or 9.8?\"\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"deepseek-ai/deepseek-r1\",\n",
    "    messages=[{\"role\": \"user\", \"content\": f\"Please reason step by step, and put your final answer within \\\\boxed{{}}. {problem}\"}],\n",
    "    temperature=0.6,\n",
    "    top_p=0.7,\n",
    "    max_tokens=32768,\n",
    "    timeout=1000,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in completion:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're ready to generate reasoning traces using DeepSeek-R1 for the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The prompt template recommended by DeepSeek for math problems\n",
    "PROMPT_TEMPLATE = \"Please reason step by step, and put your final answer within \\\\boxed{{}}. {problem}\"\n",
    "\n",
    "def process_streaming_response(completion):\n",
    "    \"\"\"Process the streaming response from the R1 model\"\"\"\n",
    "    reasoning_trace = \"\"\n",
    "    try:\n",
    "        for chunk in completion:\n",
    "            if chunk.choices[0].delta.content is not None:\n",
    "                reasoning_trace += chunk.choices[0].delta.content\n",
    "        return reasoning_trace\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        return reasoning_trace\n",
    "\n",
    "def distill_data_from_r1(example):\n",
    "    problem = example[\"problem\"]\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"deepseek-ai/deepseek-r1\",\n",
    "        messages=[{\"role\": \"user\", \"content\": PROMPT_TEMPLATE.format(problem=problem)}],\n",
    "        temperature=0.6,\n",
    "        top_p=0.7,\n",
    "        max_tokens=32768,\n",
    "        timeout=10000,\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    reasoning_trace = process_streaming_response(completion)\n",
    "    return {**example, \"reasoning_trace\": reasoning_trace}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To speed up the process, we only use 2 examples here\n",
    "sample_dataset = dataset.select(range(2))\n",
    "\n",
    "# You can set num_proc to speed up the process\n",
    "sample_dataset = sample_dataset.map(distill_data_from_r1, num_proc=1, desc=\"Distilling reasoning traces from R1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset['reasoning_trace']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Post-Process Distilled Data\n",
    "\n",
    "After generating data, we should filter out any low-quality reasoning data. We can establish some filtering rules, such as:\n",
    "- Whether the language in the reasoning trace meets requirements\n",
    "- Whether the reasoning trace format is correct, i.e., wrapping the thinking process in `<think></think>` tags before giving the final answer\n",
    "- Whether the answer given in the reasoning trace is correct\n",
    "- Other filtering rules mentioned in the R1 paper\n",
    "    - Long paragraphs\n",
    "    - Containing Code blocks\n",
    "\n",
    "In this tutorial, we will only verify the format and the correctness of the answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from latex2sympy2_extended import NormalizationConfig\n",
    "from math_verify import LatexExtractionConfig, parse, verify\n",
    "\n",
    "\n",
    "def check_format(reasoning_trace):\n",
    "    pattern = r\"^<think>.*?</think>\"\n",
    "    if not re.match(pattern, reasoning_trace, re.DOTALL | re.MULTILINE):\n",
    "        return False\n",
    "    # check if all tags only appear once\n",
    "    tags = [\"<think>\", \"</think>\"]\n",
    "    for tag in tags:\n",
    "        if reasoning_trace.count(tag) != 1:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# We use math_verify to check if the answer is mathematically equivalent to the ground truth\n",
    "def calculate_answer(reasoning_trace, ground_truth):\n",
    "    \"\"\"Check if the answer is the same as the ground truth.\"\"\"\n",
    "    answer_parsed = parse(\n",
    "        reasoning_trace,\n",
    "        extraction_config=[\n",
    "            LatexExtractionConfig(\n",
    "                normalization_config=NormalizationConfig(\n",
    "                    nits=False,\n",
    "                    malformed_operators=False,\n",
    "                    basic_latex=True,\n",
    "                    equations=True,\n",
    "                    boxed=True,\n",
    "                    units=True,\n",
    "                ),\n",
    "                # Ensures that boxed is tried first\n",
    "                boxed_match_priority=0,\n",
    "                try_extract_without_anchor=False,\n",
    "            )\n",
    "        ],\n",
    "        extraction_mode=\"first_match\",\n",
    "    )\n",
    "\n",
    "    return verify(answer_parsed, ground_truth)\n",
    "\n",
    "def filter_reasoning_trace(example):\n",
    "    reasoning_trace = example[\"reasoning_trace\"]\n",
    "    ground_truth = example[\"answer\"]\n",
    "    if not check_format(reasoning_trace):\n",
    "        return {**example, \"filtered\": True, \"filtered_reason\": \"INVALID_FORMAT\"}\n",
    "    if not calculate_answer(reasoning_trace, ground_truth):\n",
    "        return {**example, \"filtered\": True, \"filtered_reason\": \"INCORRECT_ANSWER\"}\n",
    "    return {**example, \"filtered\": False, \"filtered_reason\": \"VALID\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset = sample_dataset.map(filter_reasoning_trace, desc=\"Filtering reasoning traces\")\n",
    "\n",
    "# filter out the invalid reasoning traces\n",
    "filtered_dataset = sample_dataset.filter(lambda x: not x[\"filtered\"])\n",
    "\n",
    "# save the filtered dataset\n",
    "filtered_dataset.save_to_disk(\"filtered_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Due to the randomness of the reasoning process, we can run the above process multiple times to generate multiple reasoning traces for each question. Then, we can apply quality filtering to construct the distilled dataset.\n",
    "\n",
    "After collecting the distilled dataset, you can refer to [the qwen2 distillation notebook](./qwen2_distill_nemo.ipynb) to train your model using this dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
