{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Distillation Pipeline to Distill DeepSeek-R1 into Qwen model with NeNo 2.0 Framework\n",
    "\n",
    "In the field of LLMs, reasoning models leverage deep thinking capabilities to significantly enhance model performance across complex scenarios. According to the [DeepSeek-R1](https://arxiv.org/abs/2501.12948) paper, the reasoning pattern of larger models can be distilled into smaller models. Specifically, we can distill long-chain-of-thought (long-CoT) data that includes reasoning processes from DeepSeek-R1 and directly fine-tune open-source models like Qwen and Llama. This straightforward distillation method significantly enhances the reasoning abilities of smaller models.\n",
    "\n",
    "To demonstrate the complete distillation process, we have prepared three notebooks that cover how to distill reasoning data from DeepSeek-R1 using the NIM API, how to train models using the distilled data, and how to evaluate the model.\n",
    "\n",
    "\n",
    "- [1.generate_reasoning_data.ipynb](./1.generate_reasoning_data.ipynb) demonstrates how to distill reasoning data from DeepSeek-R1 using the NIM API. \n",
    "- [2.qwen2_distill_nemo.ipynb](./2.qwen2_distill_nemo.ipynb) (⭐) shows how to train open-source models using the distilled data.\n",
    "- [3.evaluation.ipynb](./3.evaluation.ipynb) shows how the evaluate the model.\n",
    "\n",
    "\n",
    "\n",
    "This notebook is part 2 of the series, and it demonstrates how to distill the reasoning ability of DeepSeek-R1 into the Qwen model using the NeMo Framework. The training is based on a custom dataset [Bespoke-Stratos-17k](https://huggingface.co/datasets/bespokelabs/Bespoke-Stratos-17k) distilled from deepseek-R1, which includes 17k reasoning chains in mathematics, code, Olympiads, science, and puzzles. You can also use the [generate_reasoning_data.ipynb](./generate_reasoning_data.ipynb) to generate your own reasoning data.\n",
    "\n",
    "\n",
    "## Preparation\n",
    "We use NeMo 2.0 and NeMo-Run to fine-tune (SFT) the Qwen model. You can start and enter the container `nvcr.io/nvidia/nemo:25.04`.\n",
    "\n",
    "\n",
    "## Step-By-Step Instructions\n",
    "\n",
    "This notebook contains five steps:\n",
    "\n",
    "1. Download the Qwen model and convert it to NeMo 2.0 format.\n",
    "2. Prepare the fine-tuning data.\n",
    "3. Fine-tune the Qwen model with NeMo 2.0 and NeMo-Run.\n",
    "4. Evaluate the model.\n",
    "5. Convert the output model from NeMo 2.0 to HF format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Download Qwen and Convert to NeMo 2.0 Format\n",
    "\n",
    "First, download the Qwen-2.5-7B-Instruct model from the Hugging Face model hub and convert it to NeMo format.\n",
    "\n",
    "We use the `llm.import_ckpt` API to download the model using `hf://<huggingface_model_id>` and convert it to NeMo 2.0 format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[NeMo W 2025-07-22 02:31:43 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "      warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─ </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Entering Experiment nemo.collections.llm.api.import_ckpt with id: nemo.collections.llm.api.import_ckpt_1753151…</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ─</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─ \u001b[0m\u001b[1;35mEntering Experiment nemo.collections.llm.api.import_ckpt with id: nemo.collections.llm.api.import_ckpt_1753151…\u001b[0m\u001b[92m ─\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Log directory is: /root/.nemo_run/experiments/nemo.collections.llm.api.import_ckpt/nemo.collections.llm.api.import_ckpt_1753151506/nemo.collections.llm.api.import_ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[02:31:46] </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Launching job nemo.collections.llm.api.import_ckpt for experiment </span>                     <a href=\"file:///opt/Run/nemo_run/run/experiment.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">experiment.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/Run/nemo_run/run/experiment.py#744\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">744</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">nemo.collections.llm.api.import_ckpt</span>                                                   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                 </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[02:31:46]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;36mLaunching job nemo.collections.llm.api.import_ckpt for experiment \u001b[0m                     \u001b]8;id=954007;file:///opt/Run/nemo_run/run/experiment.py\u001b\\\u001b[2mexperiment.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=761870;file:///opt/Run/nemo_run/run/experiment.py#744\u001b\\\u001b[2m744\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m\u001b[1;36mnemo.collections.llm.api.import_ckpt\u001b[0m                                                   \u001b[2m                 \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Log directory is: /root/.nemo_run/experiments/nemo.collections.llm.api.import_ckpt/nemo.collections.llm.api.import_ckpt_1753151506/nemo.collections.llm.api.import_ckpt\n",
      "Launched app: local_persistent://nemo_run/nemo.collections.llm.api.import_ckpt-k5rd2fb0vxdl3c\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">──────────────── </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Waiting for Experiment nemo.collections.llm.api.import_ckpt_1753151506 to finish</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ─────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m──────────────── \u001b[0m\u001b[1;35mWaiting for Experiment nemo.collections.llm.api.import_ckpt_1753151506 to finish\u001b[0m\u001b[92m ─────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Experiment Status for</span> <span style=\"color: #ffaf00; text-decoration-color: #ffaf00; font-weight: bold\">nemo.collections.llm.api.import_ckpt_1753151506</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mExperiment Status for\u001b[0m \u001b[1;38;5;214mnemo.collections.llm.api.import_ckpt_1753151506\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Task 0</span>: <span style=\"color: #ffaf00; text-decoration-color: #ffaf00; font-weight: bold\">nemo.collections.llm.api.import_ckpt</span>\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Status</span>: RUNNING\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Executor</span>: LocalExecutor\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Job id</span>: nemo.collections.llm.api.import_ckpt-k5rd2fb0vxdl3c\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Local Directory</span>: /root/.nemo_run/experiments/nemo.collections.llm.api.import_ckpt/nemo.collections.llm.api.import_ckpt_1753151506/nemo.collections.llm.api.import_ckpt\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;32mTask 0\u001b[0m: \u001b[1;38;5;214mnemo.collections.llm.api.import_ckpt\u001b[0m\n",
       "- \u001b[1;32mStatus\u001b[0m: RUNNING\n",
       "- \u001b[1;32mExecutor\u001b[0m: LocalExecutor\n",
       "- \u001b[1;32mJob id\u001b[0m: nemo.collections.llm.api.import_ckpt-k5rd2fb0vxdl3c\n",
       "- \u001b[1;32mLocal Directory\u001b[0m: /root/.nemo_run/experiments/nemo.collections.llm.api.import_ckpt/nemo.collections.llm.api.import_ckpt_1753151506/nemo.collections.llm.api.import_ckpt\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Waiting for job nemo.collections.llm.api.import_ckpt-k5rd2fb0vxdl3c to finish [log=True]...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mport_ckpt/0 [NeMo W 2025-07-22 02:32:02 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "mport_ckpt/0       warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n",
      "mport_ckpt/0     \n",
      "mport_ckpt/0 Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "mport_ckpt/0 [NeMo I 2025-07-22 02:32:15 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config\n",
      "mport_ckpt/0 [NeMo I 2025-07-22 02:32:15 nemo_logging:393] Rank 0 has data parallel group : [0]\n",
      "mport_ckpt/0 [NeMo I 2025-07-22 02:32:15 nemo_logging:393] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "mport_ckpt/0 [NeMo I 2025-07-22 02:32:15 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0]]\n",
      "mport_ckpt/0 [NeMo I 2025-07-22 02:32:15 nemo_logging:393] Ranks 0 has data parallel rank: 0\n",
      "mport_ckpt/0 [NeMo I 2025-07-22 02:32:15 nemo_logging:393] Rank 0 has context parallel group: [0]\n",
      "mport_ckpt/0 [NeMo I 2025-07-22 02:32:15 nemo_logging:393] All context parallel group ranks: [[0]]\n",
      "mport_ckpt/0 [NeMo I 2025-07-22 02:32:15 nemo_logging:393] Ranks 0 has context parallel rank: 0\n",
      "mport_ckpt/0 [NeMo I 2025-07-22 02:32:15 nemo_logging:393] Rank 0 has model parallel group: [0]\n",
      "mport_ckpt/0 [NeMo I 2025-07-22 02:32:15 nemo_logging:393] All model parallel group ranks: [[0]]\n",
      "mport_ckpt/0 [NeMo I 2025-07-22 02:32:15 nemo_logging:393] Rank 0 has tensor model parallel group: [0]\n",
      "mport_ckpt/0 [NeMo I 2025-07-22 02:32:15 nemo_logging:393] All tensor model parallel group ranks: [[0]]\n",
      "mport_ckpt/0 [NeMo I 2025-07-22 02:32:15 nemo_logging:393] Rank 0 has tensor model parallel rank: 0\n",
      "mport_ckpt/0 [NeMo I 2025-07-22 02:32:15 nemo_logging:393] Rank 0 has pipeline model parallel group: [0]\n",
      "mport_ckpt/0 [NeMo I 2025-07-22 02:32:15 nemo_logging:393] Rank 0 has embedding group: [0]\n",
      "mport_ckpt/0 [NeMo I 2025-07-22 02:32:15 nemo_logging:393] All pipeline model parallel group ranks: [[0]]\n",
      "mport_ckpt/0 [NeMo I 2025-07-22 02:32:15 nemo_logging:393] Rank 0 has pipeline model parallel rank 0\n",
      "mport_ckpt/0 [NeMo I 2025-07-22 02:32:15 nemo_logging:393] All embedding group ranks: [[0]]\n",
      "mport_ckpt/0 [NeMo I 2025-07-22 02:32:15 nemo_logging:393] Rank 0 has embedding rank: 0\n",
      "mport_ckpt/0 [NeMo I 2025-07-22 02:32:15 nemo_logging:393] Use preset vocab_size: 151936, original vocab_size: 151665, dummy tokens: 271.\n",
      "mport_ckpt/0 GPU available: True (cuda), used: False\n",
      "mport_ckpt/0 TPU available: False, using: 0 TPU cores\n",
      "mport_ckpt/0 HPU available: False, using: 0 HPUs\n",
      "mport_ckpt/0 [NeMo W 2025-07-22 02:32:15 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "mport_ckpt/0     \n",
      "mport_ckpt/0 ----------------------------------------------------------------------------------------------------\n",
      "mport_ckpt/0 distributed_backend=gloo\n",
      "mport_ckpt/0 All distributed processes registered. Starting with 1 processes\n",
      "mport_ckpt/0 ----------------------------------------------------------------------------------------------------\n",
      "mport_ckpt/0 \n",
      "mport_ckpt/0 [NeMo W 2025-07-22 02:32:15 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py:1090: `trainer.init_module` cannot fully support proper instantiation of your model with the `MegatronStrategy` strategy. Please instantiate your model inside the`LightningModule.configure_model` hook instead\n",
      "mport_ckpt/0     \n",
      "mport_ckpt/0 [NeMo I 2025-07-22 02:32:16 nemo_logging:393] Using FullyParallelSaveStrategyWrapper(torch_dist, 1) dist-ckpt save strategy.\n",
      "mport_ckpt/0 [NeMo W 2025-07-22 02:32:16 nemo_logging:405] Could not copy Trainer's 'max_steps' to LR scheduler's 'max_steps'. If you are not using an LR scheduler, this warning can safely be ignored.\n",
      "mport_ckpt/0 [NeMo I 2025-07-22 02:32:17 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 0 : Start time: 1753151536.602s : Save duration: 0.462s\n",
      "mport_ckpt/0 [NeMo I 2025-07-22 02:32:21 nemo_logging:393] Successfully saved checkpoint from iteration       0 to nemo/models/Qwen/Qwen2.5-1.5B-Instruct\n",
      "mport_ckpt/0 [NeMo I 2025-07-22 02:32:21 nemo_logging:393] Async finalization time took 4.351 s\n",
      "mport_ckpt/0 Converted Qwen model to Nemo, model saved to nemo/models/Qwen/Qwen2.5-1.5B-Instruct\n",
      "mport_ckpt/0 \u001b[32m✓ Checkpoint imported to nemo/models/Qwen/Qwen2.\u001b[0m\u001b[1;32m5\u001b[0m\u001b[32m-\u001b[0m\u001b[1;32m1.\u001b[0m\u001b[32m5B-Instruct\u001b[0m\n",
      "mport_ckpt/0 \u001b[1;34mImported Checkpoint\u001b[0m\n",
      "mport_ckpt/0 ├── \u001b[1;36mcontext/\u001b[0m\n",
      "mport_ckpt/0 │   ├── \u001b[1;36martifacts/\u001b[0m\n",
      "mport_ckpt/0 │   │   └── \u001b[33mgeneration_config.json\u001b[0m\n",
      "mport_ckpt/0 │   ├── \u001b[1;36mnemo_tokenizer/\u001b[0m\n",
      "mport_ckpt/0 │   │   ├── \u001b[33madded_tokens.json\u001b[0m\n",
      "mport_ckpt/0 │   │   ├── \u001b[37mmerges.txt\u001b[0m\n",
      "mport_ckpt/0 │   │   ├── \u001b[33mspecial_tokens_map.json\u001b[0m\n",
      "mport_ckpt/0 │   │   ├── \u001b[33mtokenizer.json\u001b[0m\n",
      "mport_ckpt/0 │   │   ├── \u001b[33mtokenizer_config.json\u001b[0m\n",
      "mport_ckpt/0 │   │   └── \u001b[33mvocab.json\u001b[0m\n",
      "mport_ckpt/0 │   ├── \u001b[33mio.json\u001b[0m\n",
      "mport_ckpt/0 │   └── \u001b[37mmodel.yaml\u001b[0m\n",
      "mport_ckpt/0 └── \u001b[1;36mweights/\u001b[0m\n",
      "mport_ckpt/0     ├── \u001b[37m.metadata\u001b[0m\n",
      "mport_ckpt/0     ├── \u001b[37m__0_0.distcp\u001b[0m\n",
      "mport_ckpt/0     ├── \u001b[37m__0_1.distcp\u001b[0m\n",
      "mport_ckpt/0     ├── \u001b[35mcommon.pt\u001b[0m\n",
      "mport_ckpt/0     └── \u001b[33mmetadata.json\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job nemo.collections.llm.api.import_ckpt-k5rd2fb0vxdl3c finished: SUCCEEDED\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># The experiment was run with the following tasks: ['nemo.collections.llm.api.import_ckpt']</span><span style=\"background-color: #272822\">                        </span>\n",
       "<span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># You can inspect and reconstruct this experiment at a later point in time using:</span><span style=\"background-color: #272822\">                                  </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> run</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">Experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">from_id(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"nemo.collections.llm.api.import_ckpt_1753151506\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                             </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">status() </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Gets the overall status</span><span style=\"background-color: #272822\">                                                                      </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">logs(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"nemo.collections.llm.api.import_ckpt\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">) </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Gets the log for the provided task</span><span style=\"background-color: #272822\">                       </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">cancel(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"nemo.collections.llm.api.import_ckpt\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">) </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Cancels the provided task if still running</span><span style=\"background-color: #272822\">             </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;149;144;119;48;2;39;40;34m# The experiment was run with the following tasks: ['nemo.collections.llm.api.import_ckpt']\u001b[0m\u001b[48;2;39;40;34m                        \u001b[0m\n",
       "\u001b[38;2;149;144;119;48;2;39;40;34m# You can inspect and reconstruct this experiment at a later point in time using:\u001b[0m\u001b[48;2;39;40;34m                                  \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mrun\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mExperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfrom_id\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mnemo.collections.llm.api.import_ckpt_1753151506\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                             \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mstatus\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Gets the overall status\u001b[0m\u001b[48;2;39;40;34m                                                                      \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlogs\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mnemo.collections.llm.api.import_ckpt\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Gets the log for the provided task\u001b[0m\u001b[48;2;39;40;34m                       \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcancel\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mnemo.collections.llm.api.import_ckpt\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Cancels the provided task if still running\u001b[0m\u001b[48;2;39;40;34m             \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># You can inspect this experiment at a later point in time using the CLI as well:</span><span style=\"background-color: #272822\">                                  </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">nemo experiment status nemo.collections.llm.api.import_ckpt_1753151506</span><span style=\"background-color: #272822\">                                             </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">nemo experiment logs nemo.collections.llm.api.import_ckpt_1753151506 </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">0</span><span style=\"background-color: #272822\">                                             </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">nemo experiment cancel nemo.collections.llm.api.import_ckpt_1753151506 </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">0</span><span style=\"background-color: #272822\">                                           </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;149;144;119;48;2;39;40;34m# You can inspect this experiment at a later point in time using the CLI as well:\u001b[0m\u001b[48;2;39;40;34m                                  \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mnemo\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mstatus\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnemo.collections.llm.api.import_ckpt_1753151506\u001b[0m\u001b[48;2;39;40;34m                                             \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mnemo\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlogs\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnemo.collections.llm.api.import_ckpt_1753151506\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m0\u001b[0m\u001b[48;2;39;40;34m                                             \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mnemo\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcancel\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnemo.collections.llm.api.import_ckpt_1753151506\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m0\u001b[0m\u001b[48;2;39;40;34m                                           \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nemo_run as run\n",
    "import lightning.pytorch as pl\n",
    "from nemo.collections import llm\n",
    "\n",
    "# llm.import_ckpt is the nemo2 API for converting Hugging Face checkpoint to NeMo format\n",
    "# example python usage:\n",
    "# llm.import_ckpt(model=llm.llama3_8b.model(), source=\"hf://meta-llama/Meta-Llama-3-8B\")\n",
    "#\n",
    "# We use run.Partial to configure this function\n",
    "def configure_checkpoint_conversion():\n",
    "    return run.Partial(\n",
    "        llm.import_ckpt,\n",
    "        model=llm.qwen25_1p5b.model(),\n",
    "        source=\"hf://Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "        overwrite=True,\n",
    "        output_path=\"./nemo/models/Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "    )\n",
    "\n",
    "# configure your function\n",
    "import_ckpt = configure_checkpoint_conversion()\n",
    "# define your executor\n",
    "local_executor = run.LocalExecutor()\n",
    "\n",
    "# run your experiment\n",
    "run.run(import_ckpt, executor=local_executor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Prepare Fine-tuning Data\n",
    "\n",
    "[Bespoke-Stratos-17k](https://huggingface.co/datasets/bespokelabs/Bespoke-Stratos-17k) is an open-souced dataset created by Bespoke Labs. It contains 17k reasoning questions from math, code, Olympiads, science and puzzle areas, which is distilled from DeepSeek-R1.\n",
    "\n",
    "Each case includes a question, a reasoning chain, and an answer. There is an example:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"system\": \"Your role as an assistant involves thoroughly exploring questions through a systematic long thinking process before providing the final precise and accurate solutions. This requires engaging in a comprehensive cycle of analysis, summarizing, exploration, reassessment, reflection, backtracing, and iteration to develop well-considered thinking process. Please structure your response into two main sections: Thought and Solution. In the Thought section, detail your reasoning process using the specified format: <|begin_of_thought|> {thought with steps separated with '\\\\n\\\\n'} <|end_of_thought|> Each step should include detailed considerations such as analisying questions, summarizing relevant findings, brainstorming new ideas, verifying the accuracy of the current steps, refining any errors, and revisiting previous steps. In the Solution section, based on various attempts, explorations, and reflections from the Thought section, systematically present the final solution that you deem correct. The solution should remain a logical, accurate, concise expression style and detail necessary step needed to reach the conclusion, formatted as follows: <|begin_of_solution|> {final formatted, precise, and clear solution} <|end_of_solution|> Now, try to solve the following question through the above guidelines:\",\n",
    "    \"conversations\": [\n",
    "      {\n",
    "        \"from\": \"user\",\n",
    "        \"value\": \"Return your final response within \\\\boxed{}. Two counterfeit coins of equal weight are mixed with $8$ identical genuine coins. The weight of each of the counterfeit coins is different from the weight of each of the genuine coins. A pair of coins is selected at random without replacement from the $10$ coins. A second pair is selected at random without replacement from the remaining $8$ coins. The combined weight of the first pair is equal to the combined weight of the second pair. What is the probability that all $4$ selected coins are genuine?\\n$\\\\textbf{(A)}\\\\ \\\\frac{7}{11}\\\\qquad\\\\textbf{(B)}\\\\ \\\\frac{9}{13}\\\\qquad\\\\textbf{(C)}\\\\ \\\\frac{11}{15}\\\\qquad\\\\textbf{(D)}\\\\ \\\\frac{15}{19}\\\\qquad\\\\textbf{(E)}\\\\ \\\\frac{15}{16}$\"\n",
    "      },\n",
    "      {\n",
    "        \"from\": \"assistant\",\n",
    "        \"value\": \"<|begin_of_thought|>\\n\\nOkay, so I need to solve this probability problem. Let me read it again and make sure I understand what's being asked. \\n\\nThere are two counterfeit coins that have the same weight, and they're mixed with 8 genuine coins. ...\n",
    "        ...the probability that all four selected coins are genuine is \\\\(\\\\boxed{D}\\\\).\\n\\n<|end_of_solution|>\"\n",
    "      }\n",
    "    ],\n",
    "  },\n",
    "  ```\n",
    "\n",
    "  To use this dataset, we need to write a custom dataloader \"BespokeDataModule\" to load the data from the dataset and support chat format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing bespoke.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile bespoke.py\n",
    "\n",
    "import json\n",
    "import shutil\n",
    "from typing import TYPE_CHECKING, Any, Dict, List, Optional\n",
    "\n",
    "# import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "from nemo.collections.llm.gpt.data.core import get_dataset_root\n",
    "from nemo.collections.llm.gpt.data.fine_tuning import FineTuningDataModule\n",
    "from nemo.lightning.io.mixin import IOMixin\n",
    "from nemo.utils import logging\n",
    "\n",
    "from functools import lru_cache\n",
    "\n",
    "from nemo.collections.llm.gpt.data.core import create_sft_dataset\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from nemo.collections.common.tokenizers import TokenizerSpec\n",
    "    from nemo.collections.llm.gpt.data.packed_sequence import PackedSequenceSpecs\n",
    "\n",
    "\n",
    "class BespokeDataModule(FineTuningDataModule, IOMixin):\n",
    "    \"\"\"A data module for fine-tuning on the Bespoke dataset.\n",
    "\n",
    "    This class inherits from the `FineTuningDataModule` class and is specifically designed for fine-tuning models on the\n",
    "    \"bespokelabs/Bespoke-Stratos-17k\" dataset. It handles data download, preprocessing, splitting, and preparing the data\n",
    "    in a format suitable for training, validation, and testing.\n",
    "\n",
    "    Args:\n",
    "        force_redownload (bool, optional): Whether to force re-download the dataset even if it exists locally. Defaults to False.\n",
    "        delete_raw (bool, optional): Whether to delete the raw downloaded dataset after preprocessing. Defaults to True.\n",
    "        See FineTuningDataModule for the other args\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_length: int = 2048,\n",
    "        tokenizer: Optional[\"TokenizerSpec\"] = None,\n",
    "        micro_batch_size: int = 4,\n",
    "        global_batch_size: int = 8,\n",
    "        rampup_batch_size: Optional[List[int]] = None,\n",
    "        force_redownload: bool = False,\n",
    "        delete_raw: bool = True,\n",
    "        seed: int = 1234,\n",
    "        memmap_workers: int = 1,\n",
    "        num_workers: int = 8,\n",
    "        pin_memory: bool = True,\n",
    "        persistent_workers: bool = False,\n",
    "        packed_sequence_specs: Optional[\"PackedSequenceSpecs\"] = None,\n",
    "        dataset_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        dataset_root: str = \"./bespoke\",\n",
    "    ):\n",
    "        self.force_redownload = force_redownload\n",
    "        self.delete_raw = delete_raw\n",
    "\n",
    "        super().__init__(\n",
    "            dataset_root=dataset_root,\n",
    "            seq_length=seq_length,\n",
    "            tokenizer=tokenizer,\n",
    "            micro_batch_size=micro_batch_size,\n",
    "            global_batch_size=global_batch_size,\n",
    "            rampup_batch_size=rampup_batch_size,\n",
    "            seed=seed,\n",
    "            memmap_workers=memmap_workers,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory,\n",
    "            persistent_workers=persistent_workers,\n",
    "            packed_sequence_specs=packed_sequence_specs,\n",
    "            dataset_kwargs=dataset_kwargs,\n",
    "        )\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        # if train file is specified, no need to do anything\n",
    "        if not self.train_path.exists() or self.force_redownload:\n",
    "            dset = self._download_data()\n",
    "            self._preprocess_and_split_data(dset)\n",
    "        super().prepare_data()\n",
    "\n",
    "    def _download_data(self):\n",
    "        logging.info(f\"Downloading {self.__class__.__name__}...\")\n",
    "        return load_dataset(\n",
    "            \"bespokelabs/Bespoke-Stratos-17k\",\n",
    "            cache_dir=str(self.dataset_root),\n",
    "            download_mode=\"force_redownload\" if self.force_redownload else None,\n",
    "        )\n",
    "\n",
    "    def _preprocess_and_split_data(self, dset, train_ratio: float = 0.80, val_ratio: float = 0.15):\n",
    "        logging.info(f\"Preprocessing {self.__class__.__name__} to jsonl format and splitting...\")\n",
    "\n",
    "        test_ratio = 1 - train_ratio - val_ratio\n",
    "        save_splits = {}\n",
    "        dataset = dset.get('train')\n",
    "        split_dataset = dataset.train_test_split(test_size=val_ratio + test_ratio, seed=self.seed)\n",
    "        split_dataset2 = split_dataset['test'].train_test_split(\n",
    "            test_size=test_ratio / (val_ratio + test_ratio), seed=self.seed\n",
    "        )\n",
    "        save_splits['training'] = split_dataset['train']\n",
    "        save_splits['validation'] = split_dataset2['train']\n",
    "        save_splits['test'] = split_dataset2['test']\n",
    "\n",
    "        print(\"len training: \", len(save_splits['training']))\n",
    "        print(\"len validation: \", len(save_splits['validation']))\n",
    "        print(\"len test: \", len(save_splits['test']))\n",
    "\n",
    "        for split_name, dataset in save_splits.items():\n",
    "            output_file = self.dataset_root / f\"{split_name}.jsonl\"\n",
    "            with output_file.open(\"w\", encoding=\"utf-8\") as f:\n",
    "                for example in dataset:\n",
    "                    \n",
    "                    conversations = example[\"conversations\"]\n",
    "\n",
    "                    for conversation in conversations:\n",
    "                        if conversation[\"from\"] == \"user\":\n",
    "                            conversation[\"from\"] = \"User\"\n",
    "                        elif conversation[\"from\"] == \"assistant\":\n",
    "                            conversation[\"from\"] = \"Assistant\"\n",
    "                        else:\n",
    "                            raise ValueError(f\"Unknown role: {conversation['role']}\")\n",
    "\n",
    "                    example[\"mask\"] = \"User\"\n",
    "                    example[\"type\"] = \"VALUE_TO_TEXT\"\n",
    "                    \n",
    "                    f.write(json.dumps(example) + \"\\n\")\n",
    "\n",
    "            logging.info(f\"{split_name} split saved to {output_file}\")\n",
    "\n",
    "        if self.delete_raw:\n",
    "            for p in self.dataset_root.iterdir():\n",
    "                if p.is_dir():\n",
    "                    shutil.rmtree(p)\n",
    "                elif '.jsonl' not in str(p.name):\n",
    "                    p.unlink()\n",
    "\n",
    "\n",
    "    @lru_cache\n",
    "    def _create_dataset(self, path, pack_metadata_path=None, is_test=False, **kwargs):\n",
    "        # pylint: disable=C0115,C0116\n",
    "        return create_sft_dataset(\n",
    "            path,\n",
    "            tokenizer=self.tokenizer,\n",
    "            seq_length=(self.seq_length if is_test or self.packed_sequence_size <= 0 else self.packed_sequence_size),\n",
    "            memmap_workers=self.memmap_workers,\n",
    "            seed=self.seed,\n",
    "            chat=True,\n",
    "            is_test=is_test,\n",
    "            pack_metadata_file_path=None,  # packing is not supported\n",
    "            pad_cu_seqlens=False,\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the dataloader \"BespokeDataModule\" is defined, we can configure it with the `run.Config` API. Given that long-CoT data typically contains longer sequences, we set seq_length=8192."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bespoke import BespokeDataModule\n",
    "\n",
    "def bespoke() -> run.Config[pl.LightningDataModule]:\n",
    "    return run.Config(BespokeDataModule, seq_length=8192, micro_batch_size=1, global_batch_size=32, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Fine-tune Qwen with NeMo 2.0 and NeMo-Run.\n",
    "\n",
    "In this part, we will introduce how to fine-tune the Qwen model with NeMo 2.0 and NeMo-Run. We need to configure the SFT training components, including the trainer, logger, optimizer and model, then launch the training using the `llm.finetune` API.\n",
    "\n",
    "\n",
    "#### Step 3.1: Configure SFT with NeMo 2.0\n",
    "First, we need to configure the trainer, logger, optimizer, and other components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nemo_run as run\n",
    "from nemo import lightning as nl\n",
    "from nemo.collections import llm\n",
    "from megatron.core.optimizer import OptimizerConfig\n",
    "import torch\n",
    "import lightning.pytorch as pl\n",
    "from pathlib import Path\n",
    "from nemo.collections.llm.recipes.precision.mixed_precision import bf16_mixed\n",
    "from nemo.collections.llm.recipes.log.default import tensorboard_logger\n",
    "\n",
    "\n",
    "# Configure the trainer\n",
    "# we use 1 GPUs for training and set the max_steps to 200.\n",
    "def trainer() -> run.Config[nl.Trainer]:\n",
    "    strategy = run.Config(\n",
    "        nl.MegatronStrategy,\n",
    "        tensor_model_parallel_size=1,\n",
    "    )\n",
    "    trainer = run.Config(\n",
    "        nl.Trainer,\n",
    "        devices=1,\n",
    "        max_steps=200,\n",
    "        accelerator=\"gpu\",\n",
    "        strategy=strategy,\n",
    "        plugins=bf16_mixed(),\n",
    "        log_every_n_steps=1,\n",
    "        limit_val_batches=0,\n",
    "        val_check_interval=0,\n",
    "        num_sanity_val_steps=0,\n",
    "    )\n",
    "    return trainer\n",
    "\n",
    "\n",
    "# Configure the logger\n",
    "# Here, we configure the log interval to 100 steps and save the model every 100 steps. you can change these parameters as needed.\n",
    "def logger() -> run.Config[nl.NeMoLogger]:\n",
    "    ckpt = run.Config(\n",
    "        nl.ModelCheckpoint,\n",
    "        save_last=True,\n",
    "        every_n_train_steps=10,\n",
    "        monitor=\"reduced_train_loss\",\n",
    "        save_top_k=3,\n",
    "        save_on_train_epoch_end=True,\n",
    "        save_optim_on_train_end=True,\n",
    "    )\n",
    "\n",
    "    return run.Config(\n",
    "        nl.NeMoLogger,\n",
    "        name=\"qwen_sft\",\n",
    "        log_dir=\"./logs\",\n",
    "        use_datetime_version=False,\n",
    "        ckpt=ckpt,\n",
    "        tensorboard=tensorboard_logger(name='qwen_sft'),\n",
    "        update_logger_directory=True\n",
    "    )\n",
    "\n",
    "\n",
    "# Configure the optimizer\n",
    "# We use the distributed Adam optimizer and pass in the OptimizerConfig.\n",
    "def adam_with_cosine_annealing() -> run.Config[nl.OptimizerModule]:\n",
    "    opt_cfg = run.Config(\n",
    "        OptimizerConfig,\n",
    "        optimizer=\"adam\",\n",
    "        lr=2e-5,\n",
    "        adam_beta2=0.98,\n",
    "        use_distributed_optimizer=True,\n",
    "        clip_grad=1.0,\n",
    "        bf16=True,\n",
    "    )\n",
    "    return run.Config(\n",
    "        nl.MegatronOptimizerModule,\n",
    "        config=opt_cfg\n",
    "    )\n",
    "\n",
    "\n",
    "# Configure the model\n",
    "# We use Qwen2Config1p5B to configure the model.\n",
    "def qwen() -> run.Config[pl.LightningModule]:\n",
    "    return run.Config(llm.Qwen2Model, config=run.Config(llm.Qwen2Config1P5B))\n",
    "\n",
    "# Configure the resume\n",
    "def resume() -> run.Config[nl.AutoResume]:\n",
    "    return run.Config(\n",
    "        nl.AutoResume,\n",
    "        restore_config=run.Config(nl.RestoreConfig,\n",
    "            path=\"./nemo/models/Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "        ),\n",
    "        resume_if_exists=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.2: Configure NeMo 2.0 `llm.finetune` API\n",
    "\n",
    "To use the components defined above, we can call the `llm.finetune` API and pass the components as parameters to it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_finetuning_recipe():\n",
    "    return run.Partial(\n",
    "        llm.finetune,\n",
    "        model=qwen(),\n",
    "        trainer=trainer(),\n",
    "        data=bespoke(),\n",
    "        log=logger(),\n",
    "        optim=adam_with_cosine_annealing(),\n",
    "        resume=resume(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.3: Launch Training\n",
    "\n",
    "To launch the training, we use `LocalExecutor` for executing our configured finetune function. In this example, we use 4 GPUs for training.\n",
    "\n",
    "For more details on the NeMo-Run executor, refer to [Execute NeMo Run](https://github.com/NVIDIA/NeMo-Run/blob/main/docs/source/guides/execution.md) of NeMo-Run Guides.\n",
    "\n",
    "After training starts, you can open a terminal and use the following command to launch TensorBoard:\n",
    "\n",
    "```\n",
    "tensorboard --logdir ./logs/tb_logs/qwen_sft\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─── </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Entering Experiment nemo.collections.llm.api.finetune with id: nemo.collections.llm.api.finetune_1753151667</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ───</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─── \u001b[0m\u001b[1;35mEntering Experiment nemo.collections.llm.api.finetune with id: nemo.collections.llm.api.finetune_1753151667\u001b[0m\u001b[92m ───\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Log directory is: /root/.nemo_run/experiments/nemo.collections.llm.api.finetune/nemo.collections.llm.api.finetune_1753151667/nemo.collections.llm.api.finetune\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[02:34:27] </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Launching job nemo.collections.llm.api.finetune for experiment </span>                        <a href=\"file:///opt/Run/nemo_run/run/experiment.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">experiment.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/Run/nemo_run/run/experiment.py#744\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">744</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">nemo.collections.llm.api.finetune</span>                                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                 </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[02:34:27]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;36mLaunching job nemo.collections.llm.api.finetune for experiment \u001b[0m                        \u001b]8;id=223360;file:///opt/Run/nemo_run/run/experiment.py\u001b\\\u001b[2mexperiment.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=599146;file:///opt/Run/nemo_run/run/experiment.py#744\u001b\\\u001b[2m744\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m\u001b[1;36mnemo.collections.llm.api.finetune\u001b[0m                                                      \u001b[2m                 \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Log directory is: /root/.nemo_run/experiments/nemo.collections.llm.api.finetune/nemo.collections.llm.api.finetune_1753151667/nemo.collections.llm.api.finetune\n",
      "Launched app: local_persistent://nemo_run/nemo.collections.llm.api.finetune-cq97pf9090jg5c\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">────────────────── </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Waiting for Experiment nemo.collections.llm.api.finetune_1753151667 to finish</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ──────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m────────────────── \u001b[0m\u001b[1;35mWaiting for Experiment nemo.collections.llm.api.finetune_1753151667 to finish\u001b[0m\u001b[92m ──────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Experiment Status for</span> <span style=\"color: #ffaf00; text-decoration-color: #ffaf00; font-weight: bold\">nemo.collections.llm.api.finetune_1753151667</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mExperiment Status for\u001b[0m \u001b[1;38;5;214mnemo.collections.llm.api.finetune_1753151667\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Task 0</span>: <span style=\"color: #ffaf00; text-decoration-color: #ffaf00; font-weight: bold\">nemo.collections.llm.api.finetune</span>\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Status</span>: RUNNING\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Executor</span>: LocalExecutor\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Job id</span>: nemo.collections.llm.api.finetune-cq97pf9090jg5c\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Local Directory</span>: /root/.nemo_run/experiments/nemo.collections.llm.api.finetune/nemo.collections.llm.api.finetune_1753151667/nemo.collections.llm.api.finetune\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;32mTask 0\u001b[0m: \u001b[1;38;5;214mnemo.collections.llm.api.finetune\u001b[0m\n",
       "- \u001b[1;32mStatus\u001b[0m: RUNNING\n",
       "- \u001b[1;32mExecutor\u001b[0m: LocalExecutor\n",
       "- \u001b[1;32mJob id\u001b[0m: nemo.collections.llm.api.finetune-cq97pf9090jg5c\n",
       "- \u001b[1;32mLocal Directory\u001b[0m: /root/.nemo_run/experiments/nemo.collections.llm.api.finetune/nemo.collections.llm.api.finetune_1753151667/nemo.collections.llm.api.finetune\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Waiting for job nemo.collections.llm.api.finetune-cq97pf9090jg5c to finish [log=True]...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i.finetune/0 I0722 02:34:29.755000 884 torch/distributed/run.py:646] Using nproc_per_node=1.\n",
      "i.finetune/0 I0722 02:34:29.756000 884 torch/distributed/launcher/api.py:195] Starting elastic_operator with launch configs:\n",
      "i.finetune/0 I0722 02:34:29.756000 884 torch/distributed/launcher/api.py:195]   entrypoint       : nemo_run.core.runners.fdl_runner\n",
      "i.finetune/0 I0722 02:34:29.756000 884 torch/distributed/launcher/api.py:195]   min_nodes        : 1\n",
      "i.finetune/0 I0722 02:34:29.756000 884 torch/distributed/launcher/api.py:195]   max_nodes        : 1\n",
      "i.finetune/0 I0722 02:34:29.756000 884 torch/distributed/launcher/api.py:195]   nproc_per_node   : 1\n",
      "i.finetune/0 I0722 02:34:29.756000 884 torch/distributed/launcher/api.py:195]   run_id           : 7911\n",
      "i.finetune/0 I0722 02:34:29.756000 884 torch/distributed/launcher/api.py:195]   rdzv_backend     : c10d\n",
      "i.finetune/0 I0722 02:34:29.756000 884 torch/distributed/launcher/api.py:195]   rdzv_endpoint    : localhost:0\n",
      "i.finetune/0 I0722 02:34:29.756000 884 torch/distributed/launcher/api.py:195]   rdzv_configs     : {'timeout': 900}\n",
      "i.finetune/0 I0722 02:34:29.756000 884 torch/distributed/launcher/api.py:195]   max_restarts     : 0\n",
      "i.finetune/0 I0722 02:34:29.756000 884 torch/distributed/launcher/api.py:195]   monitor_interval : 0.1\n",
      "i.finetune/0 I0722 02:34:29.756000 884 torch/distributed/launcher/api.py:195]   log_dir          : /root/.nemo_run/experiments/nemo.collections.llm.api.finetune/nemo.collections.llm.api.finetune_1753151667/nemo.collections.llm.api.finetune/nemo_run/nemo.collections.llm.api.finetune-cq97pf9090jg5c/torchelastic/nemo.collections.llm.api.finetune\n",
      "i.finetune/0 I0722 02:34:29.756000 884 torch/distributed/launcher/api.py:195]   metrics_cfg      : {}\n",
      "i.finetune/0 I0722 02:34:29.756000 884 torch/distributed/launcher/api.py:195] \n",
      "i.finetune/0 I0722 02:34:29.762000 884 torch/distributed/elastic/agent/server/api.py:860] [default] starting workers for entrypoint: python\n",
      "i.finetune/0 I0722 02:34:29.762000 884 torch/distributed/elastic/agent/server/api.py:677] [default] Rendezvous'ing worker group\n",
      "i.finetune/0 I0722 02:34:29.966000 884 torch/distributed/elastic/agent/server/api.py:525] [default] Rendezvous complete for workers. Result:\n",
      "i.finetune/0 I0722 02:34:29.966000 884 torch/distributed/elastic/agent/server/api.py:525]   restart_count=0\n",
      "i.finetune/0 I0722 02:34:29.966000 884 torch/distributed/elastic/agent/server/api.py:525]   master_addr=69765b3e9bd8\n",
      "i.finetune/0 I0722 02:34:29.966000 884 torch/distributed/elastic/agent/server/api.py:525]   master_port=46401\n",
      "i.finetune/0 I0722 02:34:29.966000 884 torch/distributed/elastic/agent/server/api.py:525]   group_rank=0\n",
      "i.finetune/0 I0722 02:34:29.966000 884 torch/distributed/elastic/agent/server/api.py:525]   group_world_size=1\n",
      "i.finetune/0 I0722 02:34:29.966000 884 torch/distributed/elastic/agent/server/api.py:525]   local_ranks=[0]\n",
      "i.finetune/0 I0722 02:34:29.966000 884 torch/distributed/elastic/agent/server/api.py:525]   role_ranks=[0]\n",
      "i.finetune/0 I0722 02:34:29.966000 884 torch/distributed/elastic/agent/server/api.py:525]   global_ranks=[0]\n",
      "i.finetune/0 I0722 02:34:29.966000 884 torch/distributed/elastic/agent/server/api.py:525]   role_world_sizes=[1]\n",
      "i.finetune/0 I0722 02:34:29.966000 884 torch/distributed/elastic/agent/server/api.py:525]   global_world_sizes=[1]\n",
      "i.finetune/0 I0722 02:34:29.966000 884 torch/distributed/elastic/agent/server/api.py:525] \n",
      "i.finetune/0 I0722 02:34:29.968000 884 torch/distributed/elastic/agent/server/api.py:685] [default] Starting worker group\n",
      "i.finetune/0 I0722 02:34:29.970000 884 torch/distributed/elastic/agent/server/local_elastic_agent.py:298] use_agent_store: True\n",
      "i.finetune/0 I0722 02:34:29.972000 884 torch/distributed/elastic/agent/server/local_elastic_agent.py:192] Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.\n",
      "i.finetune/0 I0722 02:34:29.974000 884 torch/distributed/elastic/agent/server/local_elastic_agent.py:236] Environment variable 'TORCHELASTIC_HEALTH_CHECK_PORT' not found. Do not start health check.\n",
      "i.finetune/0 [default0]:[NeMo W 2025-07-22 02:34:44 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "i.finetune/0 [default0]:      warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n",
      "i.finetune/0 [default0]:    \n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:34:45 nemo_logging:393] Experiments will be logged at logs/qwen_sft\n",
      "i.finetune/0 [default0]:GPU available: True (cuda), used: True\n",
      "i.finetune/0 [default0]:TPU available: False, using: 0 TPU cores\n",
      "i.finetune/0 [default0]:HPU available: False, using: 0 HPUs\n",
      "i.finetune/0 [default0]:[NeMo W 2025-07-22 02:34:45 nemo_logging:405] No version folders would be created under the log folder as 'resume_if_exists' is enabled.\n",
      "i.finetune/0 [default0]:[NeMo W 2025-07-22 02:34:45 nemo_logging:405] \"update_logger_directory\" is True. Overwriting tensorboard logger \"save_dir\" to logs/tb_logs\n",
      "i.finetune/0 [default0]:[NeMo W 2025-07-22 02:34:45 nemo_logging:405] The Trainer already contains a ModelCheckpoint callback. This will be overwritten.\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:34:46 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:34:46 nemo_logging:393] Rank 0 has data parallel group : [0]\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:34:46 nemo_logging:393] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:34:46 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0]]\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:34:46 nemo_logging:393] Ranks 0 has data parallel rank: 0\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:34:46 nemo_logging:393] Rank 0 has context parallel group: [0]\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:34:46 nemo_logging:393] All context parallel group ranks: [[0]]\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:34:46 nemo_logging:393] Ranks 0 has context parallel rank: 0\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:34:46 nemo_logging:393] Rank 0 has model parallel group: [0]\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:34:46 nemo_logging:393] All model parallel group ranks: [[0]]\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:34:46 nemo_logging:393] Rank 0 has tensor model parallel group: [0]\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:34:46 nemo_logging:393] All tensor model parallel group ranks: [[0]]\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:34:46 nemo_logging:393] Rank 0 has tensor model parallel rank: 0\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:34:46 nemo_logging:393] Rank 0 has pipeline model parallel group: [0]\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:34:46 nemo_logging:393] Rank 0 has embedding group: [0]\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:34:46 nemo_logging:393] All pipeline model parallel group ranks: [[0]]\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:34:46 nemo_logging:393] Rank 0 has pipeline model parallel rank 0\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:34:46 nemo_logging:393] All embedding group ranks: [[0]]\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:34:46 nemo_logging:393] Rank 0 has embedding rank: 0\n",
      "i.finetune/0 [default0]:----------------------------------------------------------------------------------------------------\n",
      "i.finetune/0 [default0]:distributed_backend=nccl\n",
      "i.finetune/0 [default0]:All distributed processes registered. Starting with 1 processes\n",
      "i.finetune/0 [default0]:----------------------------------------------------------------------------------------------------\n",
      "i.finetune/0 [default0]:\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:34:46 nemo_logging:393] Downloading BespokeDataModule...\n",
      "i.finetune/0 [default0]:\n",
      "i.finetune/0 [default0]:Generating train split:   0%|          | 0/16710 [00:00<?, ? examples/s]i.finetune/0 [default0]:\n",
      "i.finetune/0 [default0]:Generating train split:   6%|▌         | 1000/16710 [00:00<00:04, 3716.73 examples/s]i.finetune/0 [default0]:\n",
      "i.finetune/0 [default0]:Generating train split:  12%|█▏        | 2000/16710 [00:00<00:02, 5775.25 examples/s]i.finetune/0 [default0]:\n",
      "i.finetune/0 [default0]:Generating train split:  36%|███▌      | 6000/16710 [00:00<00:00, 12732.70 examples/s]i.finetune/0 [default0]:\n",
      "i.finetune/0 [default0]:Generating train split:  54%|█████▍    | 9000/16710 [00:00<00:00, 16058.78 examples/s]i.finetune/0 [default0]:\n",
      "i.finetune/0 [default0]:Generating train split:  66%|██████▌   | 11000/16710 [00:00<00:00, 12398.66 examples/s]i.finetune/0 [default0]:\n",
      "i.finetune/0 [default0]:Generating train split:  78%|███████▊  | 13000/16710 [00:01<00:00, 11471.56 examples/s]i.finetune/0 [default0]:\n",
      "i.finetune/0 [default0]:Generating train split:  90%|████████▉ | 15000/16710 [00:01<00:00, 9014.52 examples/s] i.finetune/0 [default0]:[NeMo I 2025-07-22 02:34:52 nemo_logging:393] Preprocessing BespokeDataModule to jsonl format and splitting...\n",
      "i.finetune/0 [default0]:len training:  13368\n",
      "i.finetune/0 [default0]:len validation:  2506\n",
      "i.finetune/0 [default0]:len test:  836\n",
      "i.finetune/0 [default0]:\n",
      "i.finetune/0 [default0]:Generating train split: 100%|██████████| 16710/16710 [00:01<00:00, 9606.86 examples/s]\n",
      "i.finetune/0 [default0]:Generating train split: 100%|██████████| 16710/16710 [00:01<00:00, 10222.00 examples/s]\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:34:55 nemo_logging:393] training split saved to bespoke/training.jsonl\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:34:55 nemo_logging:393] validation split saved to bespoke/validation.jsonl\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:34:55 nemo_logging:393] test split saved to bespoke/test.jsonl\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:34:56 nemo_logging:393] Use preset vocab_size: 151936, original vocab_size: 151665, dummy tokens: 271.\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:34:56 num_microbatches_calculator:228] setting number of microbatches to constant 32\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:34:56 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0 ,0): 1777088000\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:34:56 utils:532] Setting up DistributedDataParallel with config DistributedDataParallelConfig(grad_reduce_in_fp32=True, overlap_grad_reduce=False, overlap_param_gather=False, align_param_gather=False, use_distributed_optimizer=True, num_distributed_optimizer_instances=1, check_for_nan_in_grad=True, check_for_large_grads=False, bucket_size=None, pad_buckets_for_high_nccl_busbw=False, average_in_collective=False, fp8_param_gather=False, use_custom_fsdp=False, data_parallel_sharding_strategy='no_shard', gradient_reduce_div_fusion=True, suggested_communication_unit_size=None, preserve_fp32_weights=True, keep_fp8_transpose_cache_when_using_custom_fsdp=False)\n",
      "i.finetune/0 [default0]:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "i.finetune/0 [default0]:[NeMo W 2025-07-22 02:34:56 nemo_logging:405] Could not copy Trainer's 'max_steps' to LR scheduler's 'max_steps'. If you are not using an LR scheduler, this warning can safely be ignored.\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:34:56 utils:553] Number of buckets for gradient all-reduce / reduce-scatter: 1\n",
      "i.finetune/0 [default0]:    Params for bucket 1 (1777088000 elements, 1777088000 padded size):\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.27.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.25.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.21.self_attention.linear_qkv.bias\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.18.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.11.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.7.self_attention.linear_qkv.bias\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.1.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.23.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.19.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.16.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.13.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.5.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.17.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.27.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.20.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.14.self_attention.linear_qkv.bias\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.6.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.4.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.0.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.output_layer.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.22.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.16.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.12.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.8.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.1.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.27.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.23.self_attention.linear_qkv.bias\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.20.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.13.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.11.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.0.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.25.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.21.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.15.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.9.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.7.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.22.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.20.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.6.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.2.self_attention.linear_qkv.bias\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.24.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.18.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.14.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.4.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.25.self_attention.linear_qkv.bias\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.22.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.15.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.13.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.9.self_attention.linear_qkv.bias\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.1.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.embedding.word_embeddings.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.27.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.23.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.11.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.3.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.18.self_attention.linear_qkv.bias\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.24.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.22.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.15.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.9.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.8.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.4.self_attention.linear_qkv.bias\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.26.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.20.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.16.self_attention.linear_qkv.bias\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.10.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.6.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.2.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.27.self_attention.linear_qkv.bias\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.24.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.15.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.11.self_attention.linear_qkv.bias\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.3.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.1.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.25.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.19.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.16.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.13.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.9.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.5.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.17.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.26.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.24.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.20.self_attention.linear_qkv.bias\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.10.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.6.self_attention.linear_qkv.bias\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.0.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.22.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.18.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.12.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.8.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.4.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.1.self_attention.linear_qkv.bias\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.26.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.19.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.16.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.13.self_attention.linear_qkv.bias\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.5.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.3.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.2.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.27.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.21.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.17.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.15.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.11.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.7.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.0.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.26.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.22.self_attention.linear_qkv.bias\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.19.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.18.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.12.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.10.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.8.self_attention.linear_qkv.bias\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.24.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.20.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.17.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.14.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.6.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.21.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.19.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.15.self_attention.linear_qkv.bias\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.7.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.5.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.1.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.23.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.17.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.13.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.3.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.18.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.24.self_attention.linear_qkv.bias\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.21.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.14.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.12.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.26.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.22.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.10.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.8.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.2.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.23.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.21.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.14.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.7.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.3.self_attention.linear_qkv.bias\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.25.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.19.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.15.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.9.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.5.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.26.self_attention.linear_qkv.bias\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.23.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.17.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.14.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.10.self_attention.linear_qkv.bias\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.2.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.24.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.12.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.4.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.25.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.23.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.19.self_attention.linear_qkv.bias\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.16.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.9.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.8.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.5.self_attention.linear_qkv.bias\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.27.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.21.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.17.self_attention.linear_qkv.bias\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.11.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.7.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.3.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.0.self_attention.linear_qkv.bias\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.final_layernorm.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.25.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.18.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.12.self_attention.linear_qkv.bias\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.4.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.2.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.26.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.20.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.16.mlp.linear_fc2.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.14.self_attention.linear_proj.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.10.self_attention.linear_qkv.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.6.mlp.linear_fc1.weight\n",
      "i.finetune/0 [default0]:    \tmodule.decoder.layers.0.mlp.linear_fc1.layer_norm_weight\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:34:56 utils:532] Setting up optimizer with config OptimizerConfig(optimizer='adam', lr=2e-05, min_lr=None, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=True, params_dtype=torch.bfloat16, use_precision_aware_optimizer=False, main_grads_dtype=torch.float32, main_params_dtype=torch.float32, exp_avg_dtype=torch.float32, exp_avg_sq_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.98, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=True, overlap_param_gather_with_optimizer_step=False, optimizer_cpu_offload=False, optimizer_offload_fraction=0.0, use_torch_optimizer_for_cpu_offload=False, overlap_cpu_optimizer_d2h_h2d=False, pin_cpu_grads=True, pin_cpu_params=True, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=False, timers=None, config_logger_dir='')\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:34:56 nemo_logging:393] Doing selective restore from RestoreConfig(path='nemo/models/Qwen/Qwen2.5-1.5B-Instruct', adapter_path=None, load_model_state=True, load_optim_state=False, load_artifacts=True)\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:34:56 nemo_logging:393] Using <megatron.core.dist_checkpointing.strategies.fully_parallel.FullyParallelLoadStrategyWrapper object at 0x7de2807965d0> dist-ckpt load strategy.\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:34:58 nemo_logging:393] Global Checkpoint Load : Rank : 0 : Start time : 1753151696.296s : Time spent in load_checkpoint: 2.047s\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:34:58 nemo_logging:393] Restoring model weights from RestoreConfig(path='nemo/models/Qwen/Qwen2.5-1.5B-Instruct', adapter_path=None, load_model_state=True, load_optim_state=False, load_artifacts=True)\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:34:58 nemo_logging:393] Finished restoring from RestoreConfig(path='nemo/models/Qwen/Qwen2.5-1.5B-Instruct', adapter_path=None, load_model_state=True, load_optim_state=False, load_artifacts=True), cleaning up.\n",
      "i.finetune/0 [default0]:\n",
      "i.finetune/0 [default0]:  | Name   | Type | Params | Mode \n",
      "i.finetune/0 [default0]:----------------------------------------\n",
      "i.finetune/0 [default0]:0 | module | DDP  | 1.8 B  | train\n",
      "i.finetune/0 [default0]:----------------------------------------\n",
      "i.finetune/0 [default0]:1.8 B     Trainable params\n",
      "i.finetune/0 [default0]:0         Non-trainable params\n",
      "i.finetune/0 [default0]:1.8 B     Total params\n",
      "i.finetune/0 [default0]:7,108.352 Total estimated model params size (MB)\n",
      "i.finetune/0 [default0]:571       Modules in train mode\n",
      "i.finetune/0 [default0]:0         Modules in eval mode\n",
      "i.finetune/0 [default0]:[rank: 0] Received SIGTERM: 15\n",
      "i.finetune/0 [default0]:[NeMo W 2025-07-22 02:34:59 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=25` in the `DataLoader` to improve performance.\n",
      "i.finetune/0 [default0]:    \n",
      "i.finetune/0 [default0]:[NeMo W 2025-07-22 02:35:47 rerun_state_machine:1264] Implicit initialization of Rerun State Machine!\n",
      "i.finetune/0 [default0]:[NeMo W 2025-07-22 02:35:47 rerun_state_machine:239] RerunStateMachine initialized in mode RerunMode.DISABLED\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 0/29 | lr: 2e-05 | global_batch_size: 32 | global_step: 0 | reduced_train_loss: 0.9302\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 1/29 | lr: 2e-05 | global_batch_size: 32 | global_step: 1 | reduced_train_loss: 0.899 | consumed_samples: 64\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 2/29 | lr: 2e-05 | global_batch_size: 32 | global_step: 2 | reduced_train_loss: 0.7845 | consumed_samples: 96\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 3/29 | lr: 2e-05 | global_batch_size: 32 | global_step: 3 | reduced_train_loss: 0.9002 | consumed_samples: 128\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 4/29 | lr: 2e-05 | global_batch_size: 32 | global_step: 4 | reduced_train_loss: 0.8248 | consumed_samples: 160\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 5/29 | lr: 2e-05 | global_batch_size: 32 | global_step: 5 | reduced_train_loss: 0.7895 | consumed_samples: 192\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 6/29 | lr: 2e-05 | global_batch_size: 32 | global_step: 6 | reduced_train_loss: 0.821 | consumed_samples: 224\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 7/29 | lr: 2e-05 | global_batch_size: 32 | global_step: 7 | reduced_train_loss: 0.7179 | consumed_samples: 256\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 8/29 | lr: 2e-05 | global_batch_size: 32 | global_step: 8 | reduced_train_loss: 0.7724 | consumed_samples: 288\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 9/29 | lr: 2e-05 | global_batch_size: 32 | global_step: 9 | reduced_train_loss: 0.7508 | consumed_samples: 320\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:37:43 nemo_logging:393] Using FullyParallelSaveStrategyWrapper(torch_dist, 1) dist-ckpt save strategy.\n",
      "i.finetune/0 [default0]:Epoch 0, global step 9: 'reduced_train_loss' reached 0.75084 (best 0.75084), saving model to 'logs/qwen_sft/checkpoints/qwen_sft--reduced_train_loss=0.7508-epoch=0-consumed_samples=320.0.ckpt' as top 3\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:37:43 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 9 : Start time: 1753151863.156s : Save duration: 0.161s\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:37:53 nemo_logging:393] Scheduled async checkpoint save for logs/qwen_sft/checkpoints/qwen_sft--reduced_train_loss=0.7508-epoch=0-consumed_samples=320.0.ckpt\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:37:54 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 9 : Start time: 1753151873.831s : Save duration: 0.589s\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:37:54 nemo_logging:393] Scheduled async checkpoint save for logs/qwen_sft/checkpoints/qwen_sft--reduced_train_loss=0.7508-epoch=0-consumed_samples=320.0-last.ckpt\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:37:54 nemo_logging:393] Async finalization time took 0.001 s\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 10/29 | lr: 2e-05 | global_batch_size: 32 | global_step: 10 | reduced_train_loss: 0.8177 | consumed_samples: 352\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:38:07 nemo_logging:393] Async finalization time took 0.000 s\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 11/29 | lr: 2e-05 | global_batch_size: 32 | global_step: 11 | reduced_train_loss: 0.7526 | consumed_samples: 384\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:38:20 nemo_logging:393] Successfully saved checkpoint from iteration       9 to logs/qwen_sft/checkpoints/qwen_sft--reduced_train_loss=0.7508-epoch=0-consumed_samples=320.0.ckpt\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:38:20 nemo_logging:393] Async checkpoint save for step 10 (logs/qwen_sft/checkpoints/qwen_sft--reduced_train_loss=0.7508-epoch=0-consumed_samples=320.0.ckpt) finalized successfully.\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:38:20 nemo_logging:393] Async finalization time took 0.216 s\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 12/29 | lr: 2e-05 | global_batch_size: 32 | global_step: 12 | reduced_train_loss: 0.7491 | consumed_samples: 416\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:38:34 nemo_logging:393] Successfully saved checkpoint from iteration       9 to logs/qwen_sft/checkpoints/qwen_sft--reduced_train_loss=0.7508-epoch=0-consumed_samples=320.0-last.ckpt\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:38:34 nemo_logging:393] Async checkpoint save for step 10 (logs/qwen_sft/checkpoints/qwen_sft--reduced_train_loss=0.7508-epoch=0-consumed_samples=320.0-last.ckpt) finalized successfully.\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:38:34 nemo_logging:393] Async finalization time took 0.060 s\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 13/29 | lr: 2e-05 | global_batch_size: 32 | global_step: 13 | reduced_train_loss: 0.6904 | consumed_samples: 448\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 14/29 | lr: 2e-05 | global_batch_size: 32 | global_step: 14 | reduced_train_loss: 0.7135 | consumed_samples: 480\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 15/29 | lr: 2e-05 | global_batch_size: 32 | global_step: 15 | reduced_train_loss: 0.7779 | consumed_samples: 512\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 16/29 | lr: 2e-05 | global_batch_size: 32 | global_step: 16 | reduced_train_loss: 0.66 | consumed_samples: 544\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 17/29 | lr: 2e-05 | global_batch_size: 32 | global_step: 17 | reduced_train_loss: 0.674 | consumed_samples: 576\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 18/29 | lr: 2e-05 | global_batch_size: 32 | global_step: 18 | reduced_train_loss: 0.6636 | consumed_samples: 608\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 19/29 | lr: 2e-05 | global_batch_size: 32 | global_step: 19 | reduced_train_loss: 0.7058 | consumed_samples: 640\n",
      "i.finetune/0 [default0]:Epoch 0, global step 19: 'reduced_train_loss' reached 0.70582 (best 0.70582), saving model to 'logs/qwen_sft/checkpoints/qwen_sft--reduced_train_loss=0.7058-epoch=0-consumed_samples=640.0.ckpt' as top 3\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:40:09 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 19 : Start time: 1753152009.605s : Save duration: 0.198s\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:40:10 nemo_logging:393] Scheduled async checkpoint save for logs/qwen_sft/checkpoints/qwen_sft--reduced_train_loss=0.7058-epoch=0-consumed_samples=640.0.ckpt\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:40:10 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 19 : Start time: 1753152010.393s : Save duration: 0.202s\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:40:11 nemo_logging:393] Scheduled async checkpoint save for logs/qwen_sft/checkpoints/qwen_sft--reduced_train_loss=0.7058-epoch=0-consumed_samples=640.0-last.ckpt\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:40:11 nemo_logging:393] Async finalization time took 0.001 s\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 20/29 | lr: 2e-05 | global_batch_size: 32 | global_step: 20 | reduced_train_loss: 0.7447 | consumed_samples: 672\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:40:24 nemo_logging:393] Async finalization time took 0.000 s\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 21/29 | lr: 2e-05 | global_batch_size: 32 | global_step: 21 | reduced_train_loss: 0.6532 | consumed_samples: 704\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:40:38 nemo_logging:393] Successfully saved checkpoint from iteration      19 to logs/qwen_sft/checkpoints/qwen_sft--reduced_train_loss=0.7058-epoch=0-consumed_samples=640.0.ckpt\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:40:38 nemo_logging:393] Async checkpoint save for step 20 (logs/qwen_sft/checkpoints/qwen_sft--reduced_train_loss=0.7058-epoch=0-consumed_samples=640.0.ckpt) finalized successfully.\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:40:38 nemo_logging:393] Successfully saved checkpoint from iteration      19 to logs/qwen_sft/checkpoints/qwen_sft--reduced_train_loss=0.7058-epoch=0-consumed_samples=640.0-last.ckpt\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:40:38 nemo_logging:393] Async checkpoint save for step 20 (logs/qwen_sft/checkpoints/qwen_sft--reduced_train_loss=0.7058-epoch=0-consumed_samples=640.0-last.ckpt) finalized successfully.\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:40:42 nemo_logging:393] Async finalization time took 3.820 s\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 22/29 | lr: 2e-05 | global_batch_size: 32 | global_step: 22 | reduced_train_loss: 0.6063 | consumed_samples: 736\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 23/29 | lr: 2e-05 | global_batch_size: 32 | global_step: 23 | reduced_train_loss: 0.7071 | consumed_samples: 768\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 24/29 | lr: 2e-05 | global_batch_size: 32 | global_step: 24 | reduced_train_loss: 0.7553 | consumed_samples: 800\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 25/29 | lr: 2e-05 | global_batch_size: 32 | global_step: 25 | reduced_train_loss: 0.7335 | consumed_samples: 832\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 26/29 | lr: 2e-05 | global_batch_size: 32 | global_step: 26 | reduced_train_loss: 0.7436 | consumed_samples: 864\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 27/29 | lr: 2e-05 | global_batch_size: 32 | global_step: 27 | reduced_train_loss: 0.6908 | consumed_samples: 896\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 28/29 | lr: 2e-05 | global_batch_size: 32 | global_step: 28 | reduced_train_loss: 0.639 | consumed_samples: 928\n",
      "i.finetune/0 [default0]:Training epoch 0, iteration 29/29 | lr: 2e-05 | global_batch_size: 32 | global_step: 29 | reduced_train_loss: 0.6889 | consumed_samples: 960\n",
      "i.finetune/0 [default0]:Epoch 0, global step 29: 'reduced_train_loss' reached 0.68886 (best 0.68886), saving model to 'logs/qwen_sft/checkpoints/qwen_sft--reduced_train_loss=0.6889-epoch=0-consumed_samples=960.0.ckpt' as top 3\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:42:31 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 29 : Start time: 1753152151.819s : Save duration: 0.107s\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:42:32 nemo_logging:393] Scheduled async checkpoint save for logs/qwen_sft/checkpoints/qwen_sft--reduced_train_loss=0.6889-epoch=0-consumed_samples=960.0.ckpt\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:42:32 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 29 : Start time: 1753152152.472s : Save duration: 0.112s\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:42:33 nemo_logging:393] Scheduled async checkpoint save for logs/qwen_sft/checkpoints/qwen_sft--reduced_train_loss=0.6889-epoch=0-consumed_samples=960.0-last.ckpt\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:42:33 nemo_logging:393] Async finalization time took 0.001 s\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:42:33 nemo_logging:393] Async finalization time took 0.000 s\n",
      "i.finetune/0 [default0]:`Trainer.fit` stopped: `max_steps=30` reached.\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:42:33 nemo_logging:393] Pending async checkpoint saves. Finalizing them synchronously now\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:43:02 nemo_logging:393] Successfully saved checkpoint from iteration      29 to logs/qwen_sft/checkpoints/qwen_sft--reduced_train_loss=0.6889-epoch=0-consumed_samples=960.0.ckpt\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:43:02 nemo_logging:393] Async checkpoint save for step 30 (logs/qwen_sft/checkpoints/qwen_sft--reduced_train_loss=0.6889-epoch=0-consumed_samples=960.0.ckpt) finalized successfully.\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:43:03 nemo_logging:393] Successfully saved checkpoint from iteration      29 to logs/qwen_sft/checkpoints/qwen_sft--reduced_train_loss=0.6889-epoch=0-consumed_samples=960.0-last.ckpt\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:43:03 nemo_logging:393] Async checkpoint save for step 30 (logs/qwen_sft/checkpoints/qwen_sft--reduced_train_loss=0.6889-epoch=0-consumed_samples=960.0-last.ckpt) finalized successfully.\n",
      "i.finetune/0 [default0]:[NeMo I 2025-07-22 02:43:07 nemo_logging:393] Async finalization time took 34.115 s\n",
      "i.finetune/0 I0722 02:43:33.362000 884 torch/distributed/elastic/agent/server/api.py:879] [default] worker group successfully finished. Waiting 300 seconds for other agents to finish.\n",
      "i.finetune/0 I0722 02:43:33.367000 884 torch/distributed/elastic/agent/server/api.py:932] Local worker group finished (WorkerState.SUCCEEDED). Waiting 300 seconds for other agents to finish\n",
      "i.finetune/0 I0722 02:43:33.369000 884 torch/distributed/elastic/agent/server/api.py:946] Done waiting for other agents. Elapsed: 0.0002865791320800781 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job nemo.collections.llm.api.finetune-cq97pf9090jg5c finished: SUCCEEDED\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># The experiment was run with the following tasks: ['nemo.collections.llm.api.finetune']</span><span style=\"background-color: #272822\">                           </span>\n",
       "<span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># You can inspect and reconstruct this experiment at a later point in time using:</span><span style=\"background-color: #272822\">                                  </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> run</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">Experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">from_id(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"nemo.collections.llm.api.finetune_1753151667\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">status() </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Gets the overall status</span><span style=\"background-color: #272822\">                                                                      </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">logs(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"nemo.collections.llm.api.finetune\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">) </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Gets the log for the provided task</span><span style=\"background-color: #272822\">                          </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">cancel(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"nemo.collections.llm.api.finetune\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">) </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Cancels the provided task if still running</span><span style=\"background-color: #272822\">                </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;149;144;119;48;2;39;40;34m# The experiment was run with the following tasks: ['nemo.collections.llm.api.finetune']\u001b[0m\u001b[48;2;39;40;34m                           \u001b[0m\n",
       "\u001b[38;2;149;144;119;48;2;39;40;34m# You can inspect and reconstruct this experiment at a later point in time using:\u001b[0m\u001b[48;2;39;40;34m                                  \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mrun\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mExperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfrom_id\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mnemo.collections.llm.api.finetune_1753151667\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mstatus\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Gets the overall status\u001b[0m\u001b[48;2;39;40;34m                                                                      \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlogs\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mnemo.collections.llm.api.finetune\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Gets the log for the provided task\u001b[0m\u001b[48;2;39;40;34m                          \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcancel\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mnemo.collections.llm.api.finetune\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Cancels the provided task if still running\u001b[0m\u001b[48;2;39;40;34m                \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># You can inspect this experiment at a later point in time using the CLI as well:</span><span style=\"background-color: #272822\">                                  </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">nemo experiment status nemo.collections.llm.api.finetune_1753151667</span><span style=\"background-color: #272822\">                                                </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">nemo experiment logs nemo.collections.llm.api.finetune_1753151667 </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">0</span><span style=\"background-color: #272822\">                                                </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">nemo experiment cancel nemo.collections.llm.api.finetune_1753151667 </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">0</span><span style=\"background-color: #272822\">                                              </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;149;144;119;48;2;39;40;34m# You can inspect this experiment at a later point in time using the CLI as well:\u001b[0m\u001b[48;2;39;40;34m                                  \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mnemo\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mstatus\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnemo.collections.llm.api.finetune_1753151667\u001b[0m\u001b[48;2;39;40;34m                                                \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mnemo\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlogs\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnemo.collections.llm.api.finetune_1753151667\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m0\u001b[0m\u001b[48;2;39;40;34m                                                \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mnemo\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcancel\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnemo.collections.llm.api.finetune_1753151667\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m0\u001b[0m\u001b[48;2;39;40;34m                                              \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def local_executor_torchrun(nodes: int = 1, devices: int = 1) -> run.LocalExecutor:\n",
    "    # Env vars for jobs are configured here\n",
    "    env_vars = {\n",
    "        \"TORCH_NCCL_AVOID_RECORD_STREAMS\": \"1\",\n",
    "        \"NCCL_NVLS_ENABLE\": \"0\",\n",
    "        \"NVTE_DP_AMAX_REDUCE_INTERVAL\": \"0\",\n",
    "        \"NVTE_ASYNC_AMAX_REDUCTION\": \"1\",\n",
    "    }\n",
    "\n",
    "    executor = run.LocalExecutor(ntasks_per_node=devices, launcher=\"torchrun\", env_vars=env_vars)\n",
    "\n",
    "    return executor\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run.run(configure_finetuning_recipe(), executor=local_executor_torchrun())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Evaluate Model\n",
    "\n",
    "This section demonstrates how to generate inference results using the trained checkpoint. In this example, we perform inference on a single GPU. You can modify the `tensor_model_parallel_size` and `local_executor_torchrun` functions to utilize multiple GPUs for speedup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"How many r's are in the word 'strawberry'?\",\n",
    "    \"which number is larger, 9.11 or 9.8?\",\n",
    "]\n",
    "\n",
    "\n",
    "def trainer() -> run.Config[nl.Trainer]:\n",
    "    strategy = run.Config(\n",
    "        nl.MegatronStrategy,\n",
    "        tensor_model_parallel_size=1\n",
    "    )\n",
    "    trainer = run.Config(\n",
    "        nl.Trainer,\n",
    "        accelerator=\"gpu\",\n",
    "        devices=1,\n",
    "        num_nodes=1,\n",
    "        strategy=strategy,\n",
    "        plugins=bf16_mixed(),\n",
    "    )\n",
    "    return trainer\n",
    "\n",
    "\n",
    "def local_executor_torchrun(nodes: int = 1, devices: int = 1) -> run.LocalExecutor:\n",
    "    # Env vars for jobs are configured here\n",
    "    env_vars = {\n",
    "        \"TORCH_NCCL_AVOID_RECORD_STREAMS\": \"1\",\n",
    "        \"NCCL_NVLS_ENABLE\": \"0\",\n",
    "        # \"NVTE_DP_AMAX_REDUCE_INTERVAL\": \"0\",\n",
    "        # \"NVTE_ASYNC_AMAX_REDUCTION\": \"1\",\n",
    "    }\n",
    "\n",
    "    executor = run.LocalExecutor(ntasks_per_node=devices, launcher=\"torchrun\", env_vars=env_vars)\n",
    "\n",
    "    return executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─── </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Entering Experiment nemo.collections.llm.api.generate with id: nemo.collections.llm.api.generate_1753152298</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ───</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─── \u001b[0m\u001b[1;35mEntering Experiment nemo.collections.llm.api.generate with id: nemo.collections.llm.api.generate_1753152298\u001b[0m\u001b[92m ───\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Log directory is: /root/.nemo_run/experiments/nemo.collections.llm.api.generate/nemo.collections.llm.api.generate_1753152298/nemo.collections.llm.api.generate\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[02:44:58] </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Launching job nemo.collections.llm.api.generate for experiment </span>                        <a href=\"file:///opt/Run/nemo_run/run/experiment.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">experiment.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/Run/nemo_run/run/experiment.py#744\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">744</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">nemo.collections.llm.api.generate</span>                                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                 </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[02:44:58]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;36mLaunching job nemo.collections.llm.api.generate for experiment \u001b[0m                        \u001b]8;id=600624;file:///opt/Run/nemo_run/run/experiment.py\u001b\\\u001b[2mexperiment.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=320710;file:///opt/Run/nemo_run/run/experiment.py#744\u001b\\\u001b[2m744\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m\u001b[1;36mnemo.collections.llm.api.generate\u001b[0m                                                      \u001b[2m                 \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Log directory is: /root/.nemo_run/experiments/nemo.collections.llm.api.generate/nemo.collections.llm.api.generate_1753152298/nemo.collections.llm.api.generate\n",
      "Launched app: local_persistent://nemo_run/nemo.collections.llm.api.generate-srzhg6bt90z6j\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">────────────────── </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Waiting for Experiment nemo.collections.llm.api.generate_1753152298 to finish</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ──────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m────────────────── \u001b[0m\u001b[1;35mWaiting for Experiment nemo.collections.llm.api.generate_1753152298 to finish\u001b[0m\u001b[92m ──────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Experiment Status for</span> <span style=\"color: #ffaf00; text-decoration-color: #ffaf00; font-weight: bold\">nemo.collections.llm.api.generate_1753152298</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mExperiment Status for\u001b[0m \u001b[1;38;5;214mnemo.collections.llm.api.generate_1753152298\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Task 0</span>: <span style=\"color: #ffaf00; text-decoration-color: #ffaf00; font-weight: bold\">nemo.collections.llm.api.generate</span>\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Status</span>: RUNNING\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Executor</span>: LocalExecutor\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Job id</span>: nemo.collections.llm.api.generate-srzhg6bt90z6j\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Local Directory</span>: /root/.nemo_run/experiments/nemo.collections.llm.api.generate/nemo.collections.llm.api.generate_1753152298/nemo.collections.llm.api.generate\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;32mTask 0\u001b[0m: \u001b[1;38;5;214mnemo.collections.llm.api.generate\u001b[0m\n",
       "- \u001b[1;32mStatus\u001b[0m: RUNNING\n",
       "- \u001b[1;32mExecutor\u001b[0m: LocalExecutor\n",
       "- \u001b[1;32mJob id\u001b[0m: nemo.collections.llm.api.generate-srzhg6bt90z6j\n",
       "- \u001b[1;32mLocal Directory\u001b[0m: /root/.nemo_run/experiments/nemo.collections.llm.api.generate/nemo.collections.llm.api.generate_1753152298/nemo.collections.llm.api.generate\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Waiting for job nemo.collections.llm.api.generate-srzhg6bt90z6j to finish [log=True]...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i.generate/0 I0722 02:45:01.226000 1765 torch/distributed/run.py:646] Using nproc_per_node=1.\n",
      "i.generate/0 I0722 02:45:01.227000 1765 torch/distributed/launcher/api.py:195] Starting elastic_operator with launch configs:\n",
      "i.generate/0 I0722 02:45:01.227000 1765 torch/distributed/launcher/api.py:195]   entrypoint       : nemo_run.core.runners.fdl_runner\n",
      "i.generate/0 I0722 02:45:01.227000 1765 torch/distributed/launcher/api.py:195]   min_nodes        : 1\n",
      "i.generate/0 I0722 02:45:01.227000 1765 torch/distributed/launcher/api.py:195]   max_nodes        : 1\n",
      "i.generate/0 I0722 02:45:01.227000 1765 torch/distributed/launcher/api.py:195]   nproc_per_node   : 1\n",
      "i.generate/0 I0722 02:45:01.227000 1765 torch/distributed/launcher/api.py:195]   run_id           : 4619\n",
      "i.generate/0 I0722 02:45:01.227000 1765 torch/distributed/launcher/api.py:195]   rdzv_backend     : c10d\n",
      "i.generate/0 I0722 02:45:01.227000 1765 torch/distributed/launcher/api.py:195]   rdzv_endpoint    : localhost:0\n",
      "i.generate/0 I0722 02:45:01.227000 1765 torch/distributed/launcher/api.py:195]   rdzv_configs     : {'timeout': 900}\n",
      "i.generate/0 I0722 02:45:01.227000 1765 torch/distributed/launcher/api.py:195]   max_restarts     : 0\n",
      "i.generate/0 I0722 02:45:01.227000 1765 torch/distributed/launcher/api.py:195]   monitor_interval : 0.1\n",
      "i.generate/0 I0722 02:45:01.227000 1765 torch/distributed/launcher/api.py:195]   log_dir          : /root/.nemo_run/experiments/nemo.collections.llm.api.generate/nemo.collections.llm.api.generate_1753152298/nemo.collections.llm.api.generate/nemo_run/nemo.collections.llm.api.generate-srzhg6bt90z6j/torchelastic/nemo.collections.llm.api.generate\n",
      "i.generate/0 I0722 02:45:01.227000 1765 torch/distributed/launcher/api.py:195]   metrics_cfg      : {}\n",
      "i.generate/0 I0722 02:45:01.227000 1765 torch/distributed/launcher/api.py:195] \n",
      "i.generate/0 I0722 02:45:01.234000 1765 torch/distributed/elastic/agent/server/api.py:860] [default] starting workers for entrypoint: python\n",
      "i.generate/0 I0722 02:45:01.234000 1765 torch/distributed/elastic/agent/server/api.py:677] [default] Rendezvous'ing worker group\n",
      "i.generate/0 I0722 02:45:01.284000 1765 torch/distributed/elastic/agent/server/api.py:525] [default] Rendezvous complete for workers. Result:\n",
      "i.generate/0 I0722 02:45:01.284000 1765 torch/distributed/elastic/agent/server/api.py:525]   restart_count=0\n",
      "i.generate/0 I0722 02:45:01.284000 1765 torch/distributed/elastic/agent/server/api.py:525]   master_addr=69765b3e9bd8\n",
      "i.generate/0 I0722 02:45:01.284000 1765 torch/distributed/elastic/agent/server/api.py:525]   master_port=40323\n",
      "i.generate/0 I0722 02:45:01.284000 1765 torch/distributed/elastic/agent/server/api.py:525]   group_rank=0\n",
      "i.generate/0 I0722 02:45:01.284000 1765 torch/distributed/elastic/agent/server/api.py:525]   group_world_size=1\n",
      "i.generate/0 I0722 02:45:01.284000 1765 torch/distributed/elastic/agent/server/api.py:525]   local_ranks=[0]\n",
      "i.generate/0 I0722 02:45:01.284000 1765 torch/distributed/elastic/agent/server/api.py:525]   role_ranks=[0]\n",
      "i.generate/0 I0722 02:45:01.284000 1765 torch/distributed/elastic/agent/server/api.py:525]   global_ranks=[0]\n",
      "i.generate/0 I0722 02:45:01.284000 1765 torch/distributed/elastic/agent/server/api.py:525]   role_world_sizes=[1]\n",
      "i.generate/0 I0722 02:45:01.284000 1765 torch/distributed/elastic/agent/server/api.py:525]   global_world_sizes=[1]\n",
      "i.generate/0 I0722 02:45:01.284000 1765 torch/distributed/elastic/agent/server/api.py:525] \n",
      "i.generate/0 I0722 02:45:01.285000 1765 torch/distributed/elastic/agent/server/api.py:685] [default] Starting worker group\n",
      "i.generate/0 I0722 02:45:01.286000 1765 torch/distributed/elastic/agent/server/local_elastic_agent.py:298] use_agent_store: True\n",
      "i.generate/0 I0722 02:45:01.287000 1765 torch/distributed/elastic/agent/server/local_elastic_agent.py:192] Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.\n",
      "i.generate/0 I0722 02:45:01.288000 1765 torch/distributed/elastic/agent/server/local_elastic_agent.py:236] Environment variable 'TORCHELASTIC_HEALTH_CHECK_PORT' not found. Do not start health check.\n",
      "i.generate/0 [default0]:[NeMo W 2025-07-22 02:45:16 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "i.generate/0 [default0]:      warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n",
      "i.generate/0 [default0]:    \n",
      "i.generate/0 [default0]:GPU available: True (cuda), used: True\n",
      "i.generate/0 [default0]:TPU available: False, using: 0 TPU cores\n",
      "i.generate/0 [default0]:HPU available: False, using: 0 HPUs\n",
      "i.generate/0 [default0]:[NeMo I 2025-07-22 02:45:19 nemo_logging:393] Rank 0 has data parallel group : [0]\n",
      "i.generate/0 [default0]:[NeMo I 2025-07-22 02:45:19 nemo_logging:393] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "i.generate/0 [default0]:[NeMo I 2025-07-22 02:45:19 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0]]\n",
      "i.generate/0 [default0]:[NeMo I 2025-07-22 02:45:19 nemo_logging:393] Ranks 0 has data parallel rank: 0\n",
      "i.generate/0 [default0]:[NeMo I 2025-07-22 02:45:19 nemo_logging:393] Rank 0 has context parallel group: [0]\n",
      "i.generate/0 [default0]:[NeMo I 2025-07-22 02:45:19 nemo_logging:393] All context parallel group ranks: [[0]]\n",
      "i.generate/0 [default0]:[NeMo I 2025-07-22 02:45:19 nemo_logging:393] Ranks 0 has context parallel rank: 0\n",
      "i.generate/0 [default0]:[NeMo I 2025-07-22 02:45:19 nemo_logging:393] Rank 0 has model parallel group: [0]\n",
      "i.generate/0 [default0]:[NeMo I 2025-07-22 02:45:19 nemo_logging:393] All model parallel group ranks: [[0]]\n",
      "i.generate/0 [default0]:[NeMo I 2025-07-22 02:45:19 nemo_logging:393] Rank 0 has tensor model parallel group: [0]\n",
      "i.generate/0 [default0]:[NeMo I 2025-07-22 02:45:19 nemo_logging:393] All tensor model parallel group ranks: [[0]]\n",
      "i.generate/0 [default0]:[NeMo I 2025-07-22 02:45:19 nemo_logging:393] Rank 0 has tensor model parallel rank: 0\n",
      "i.generate/0 [default0]:[NeMo I 2025-07-22 02:45:19 nemo_logging:393] Rank 0 has pipeline model parallel group: [0]\n",
      "i.generate/0 [default0]:[NeMo I 2025-07-22 02:45:19 nemo_logging:393] Rank 0 has embedding group: [0]\n",
      "i.generate/0 [default0]:[NeMo I 2025-07-22 02:45:19 nemo_logging:393] All pipeline model parallel group ranks: [[0]]\n",
      "i.generate/0 [default0]:[NeMo I 2025-07-22 02:45:19 nemo_logging:393] Rank 0 has pipeline model parallel rank 0\n",
      "i.generate/0 [default0]:[NeMo I 2025-07-22 02:45:19 nemo_logging:393] All embedding group ranks: [[0]]\n",
      "i.generate/0 [default0]:[NeMo I 2025-07-22 02:45:19 nemo_logging:393] Rank 0 has embedding rank: 0\n",
      "i.generate/0 [default0]:[NeMo I 2025-07-22 02:45:19 nemo_logging:393] Use preset vocab_size: 151936, original vocab_size: 151665, dummy tokens: 271.\n",
      "i.generate/0 [default0]:----------------------------------------------------------------------------------------------------\n",
      "i.generate/0 [default0]:distributed_backend=nccl\n",
      "i.generate/0 [default0]:All distributed processes registered. Starting with 1 processes\n",
      "i.generate/0 [default0]:----------------------------------------------------------------------------------------------------\n",
      "i.generate/0 [default0]:\n",
      "i.generate/0 [default0]:[NeMo I 2025-07-22 02:45:20 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0 ,0): 1777088000\n",
      "i.generate/0 [default0]:[NeMo I 2025-07-22 02:45:20 nemo_logging:393] Doing selective restore from RestoreConfig(path='./nemo/models/Qwen/Qwen2.5-1.5B-Instruct', adapter_path=None, load_model_state=True, load_optim_state=False, load_artifacts=True)\n",
      "i.generate/0 [default0]:[NeMo I 2025-07-22 02:45:20 nemo_logging:393] Using <megatron.core.dist_checkpointing.strategies.fully_parallel.FullyParallelLoadStrategyWrapper object at 0x7382bc98de80> dist-ckpt load strategy.\n",
      "i.generate/0 [default0]:[NeMo I 2025-07-22 02:45:24 nemo_logging:393] Global Checkpoint Load : Rank : 0 : Start time : 1753152320.188s : Time spent in load_checkpoint: 4.516s\n",
      "i.generate/0 [default0]:[NeMo I 2025-07-22 02:45:24 nemo_logging:393] Restoring model weights from RestoreConfig(path='./nemo/models/Qwen/Qwen2.5-1.5B-Instruct', adapter_path=None, load_model_state=True, load_optim_state=False, load_artifacts=True)\n",
      "i.generate/0 [default0]:[NeMo I 2025-07-22 02:45:24 nemo_logging:393] Finished restoring from RestoreConfig(path='./nemo/models/Qwen/Qwen2.5-1.5B-Instruct', adapter_path=None, load_model_state=True, load_optim_state=False, load_artifacts=True), cleaning up.\n",
      "i.generate/0 [default0]:\n",
      "i.generate/0 [default0]:static requests:   0%|          | 0/2 [00:00<?, ?it/s]i.generate/0 [default0]:[NeMo I 2025-07-22 02:48:27 nemo_logging:393] Predictions written to qwen_before_sft_prediction.jsonl\n",
      "i.generate/0 [default0]:\n",
      "i.generate/0 [default0]:static requests: 100%|██████████| 2/2 [03:02<00:00, 91.32s/it]\n",
      "i.generate/0 [default0]:static requests: 100%|██████████| 2/2 [03:02<00:00, 91.32s/it]\n",
      "i.generate/0 I0722 02:48:33.163000 1765 torch/distributed/elastic/agent/server/api.py:879] [default] worker group successfully finished. Waiting 300 seconds for other agents to finish.\n",
      "i.generate/0 I0722 02:48:33.165000 1765 torch/distributed/elastic/agent/server/api.py:932] Local worker group finished (WorkerState.SUCCEEDED). Waiting 300 seconds for other agents to finish\n",
      "i.generate/0 I0722 02:48:33.167000 1765 torch/distributed/elastic/agent/server/api.py:946] Done waiting for other agents. Elapsed: 0.0003819465637207031 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job nemo.collections.llm.api.generate-srzhg6bt90z6j finished: SUCCEEDED\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># The experiment was run with the following tasks: ['nemo.collections.llm.api.generate']</span><span style=\"background-color: #272822\">                           </span>\n",
       "<span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># You can inspect and reconstruct this experiment at a later point in time using:</span><span style=\"background-color: #272822\">                                  </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> run</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">Experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">from_id(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"nemo.collections.llm.api.generate_1753152298\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">status() </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Gets the overall status</span><span style=\"background-color: #272822\">                                                                      </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">logs(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"nemo.collections.llm.api.generate\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">) </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Gets the log for the provided task</span><span style=\"background-color: #272822\">                          </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">cancel(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"nemo.collections.llm.api.generate\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">) </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Cancels the provided task if still running</span><span style=\"background-color: #272822\">                </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;149;144;119;48;2;39;40;34m# The experiment was run with the following tasks: ['nemo.collections.llm.api.generate']\u001b[0m\u001b[48;2;39;40;34m                           \u001b[0m\n",
       "\u001b[38;2;149;144;119;48;2;39;40;34m# You can inspect and reconstruct this experiment at a later point in time using:\u001b[0m\u001b[48;2;39;40;34m                                  \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mrun\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mExperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfrom_id\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mnemo.collections.llm.api.generate_1753152298\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mstatus\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Gets the overall status\u001b[0m\u001b[48;2;39;40;34m                                                                      \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlogs\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mnemo.collections.llm.api.generate\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Gets the log for the provided task\u001b[0m\u001b[48;2;39;40;34m                          \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcancel\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mnemo.collections.llm.api.generate\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Cancels the provided task if still running\u001b[0m\u001b[48;2;39;40;34m                \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># You can inspect this experiment at a later point in time using the CLI as well:</span><span style=\"background-color: #272822\">                                  </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">nemo experiment status nemo.collections.llm.api.generate_1753152298</span><span style=\"background-color: #272822\">                                                </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">nemo experiment logs nemo.collections.llm.api.generate_1753152298 </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">0</span><span style=\"background-color: #272822\">                                                </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">nemo experiment cancel nemo.collections.llm.api.generate_1753152298 </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">0</span><span style=\"background-color: #272822\">                                              </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;149;144;119;48;2;39;40;34m# You can inspect this experiment at a later point in time using the CLI as well:\u001b[0m\u001b[48;2;39;40;34m                                  \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mnemo\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mstatus\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnemo.collections.llm.api.generate_1753152298\u001b[0m\u001b[48;2;39;40;34m                                                \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mnemo\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlogs\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnemo.collections.llm.api.generate_1753152298\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m0\u001b[0m\u001b[48;2;39;40;34m                                                \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mnemo\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcancel\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnemo.collections.llm.api.generate_1753152298\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m0\u001b[0m\u001b[48;2;39;40;34m                                              \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from megatron.core.inference.common_inference_params import CommonInferenceParams\n",
    "model_path=\"./nemo/models/Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "    \n",
    "def configure_inference():\n",
    "    return run.Partial(\n",
    "        llm.generate,\n",
    "        path=model_path,\n",
    "        trainer=trainer(),\n",
    "        prompts=prompts,\n",
    "        inference_params=CommonInferenceParams(num_tokens_to_generate=512, top_p=0.6),\n",
    "        output_path=\"qwen_before_sft_prediction.jsonl\",\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run.run(configure_inference(), executor=local_executor_torchrun())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will load SFT checkpoint from: logs/qwen_sft/checkpoints/qwen_sft--reduced_train_loss=0.6889-epoch=0-consumed_samples=960.0-last\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─── </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Entering Experiment nemo.collections.llm.api.generate with id: nemo.collections.llm.api.generate_1753152524</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ───</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─── \u001b[0m\u001b[1;35mEntering Experiment nemo.collections.llm.api.generate with id: nemo.collections.llm.api.generate_1753152524\u001b[0m\u001b[92m ───\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Log directory is: /root/.nemo_run/experiments/nemo.collections.llm.api.generate/nemo.collections.llm.api.generate_1753152524/nemo.collections.llm.api.generate\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[02:48:44] </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Launching job nemo.collections.llm.api.generate for experiment </span>                        <a href=\"file:///opt/Run/nemo_run/run/experiment.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">experiment.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/Run/nemo_run/run/experiment.py#744\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">744</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">nemo.collections.llm.api.generate</span>                                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                 </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[02:48:44]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;36mLaunching job nemo.collections.llm.api.generate for experiment \u001b[0m                        \u001b]8;id=97624;file:///opt/Run/nemo_run/run/experiment.py\u001b\\\u001b[2mexperiment.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=197245;file:///opt/Run/nemo_run/run/experiment.py#744\u001b\\\u001b[2m744\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m\u001b[1;36mnemo.collections.llm.api.generate\u001b[0m                                                      \u001b[2m                 \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Log directory is: /root/.nemo_run/experiments/nemo.collections.llm.api.generate/nemo.collections.llm.api.generate_1753152524/nemo.collections.llm.api.generate\n",
      "Launched app: local_persistent://nemo_run/nemo.collections.llm.api.generate-rkt9qbwdzq9npc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">────────────────── </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Waiting for Experiment nemo.collections.llm.api.generate_1753152524 to finish</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ──────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m────────────────── \u001b[0m\u001b[1;35mWaiting for Experiment nemo.collections.llm.api.generate_1753152524 to finish\u001b[0m\u001b[92m ──────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Experiment Status for</span> <span style=\"color: #ffaf00; text-decoration-color: #ffaf00; font-weight: bold\">nemo.collections.llm.api.generate_1753152524</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mExperiment Status for\u001b[0m \u001b[1;38;5;214mnemo.collections.llm.api.generate_1753152524\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Task 0</span>: <span style=\"color: #ffaf00; text-decoration-color: #ffaf00; font-weight: bold\">nemo.collections.llm.api.generate</span>\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Status</span>: RUNNING\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Executor</span>: LocalExecutor\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Job id</span>: nemo.collections.llm.api.generate-rkt9qbwdzq9npc\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Local Directory</span>: /root/.nemo_run/experiments/nemo.collections.llm.api.generate/nemo.collections.llm.api.generate_1753152524/nemo.collections.llm.api.generate\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;32mTask 0\u001b[0m: \u001b[1;38;5;214mnemo.collections.llm.api.generate\u001b[0m\n",
       "- \u001b[1;32mStatus\u001b[0m: RUNNING\n",
       "- \u001b[1;32mExecutor\u001b[0m: LocalExecutor\n",
       "- \u001b[1;32mJob id\u001b[0m: nemo.collections.llm.api.generate-rkt9qbwdzq9npc\n",
       "- \u001b[1;32mLocal Directory\u001b[0m: /root/.nemo_run/experiments/nemo.collections.llm.api.generate/nemo.collections.llm.api.generate_1753152524/nemo.collections.llm.api.generate\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Waiting for job nemo.collections.llm.api.generate-rkt9qbwdzq9npc to finish [log=True]...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i.generate/0 I0722 02:48:46.688000 2043 torch/distributed/run.py:646] Using nproc_per_node=1.\n",
      "i.generate/0 I0722 02:48:46.689000 2043 torch/distributed/launcher/api.py:195] Starting elastic_operator with launch configs:\n",
      "i.generate/0 I0722 02:48:46.689000 2043 torch/distributed/launcher/api.py:195]   entrypoint       : nemo_run.core.runners.fdl_runner\n",
      "i.generate/0 I0722 02:48:46.689000 2043 torch/distributed/launcher/api.py:195]   min_nodes        : 1\n",
      "i.generate/0 I0722 02:48:46.689000 2043 torch/distributed/launcher/api.py:195]   max_nodes        : 1\n",
      "i.generate/0 I0722 02:48:46.689000 2043 torch/distributed/launcher/api.py:195]   nproc_per_node   : 1\n",
      "i.generate/0 I0722 02:48:46.689000 2043 torch/distributed/launcher/api.py:195]   run_id           : 9506\n",
      "i.generate/0 I0722 02:48:46.689000 2043 torch/distributed/launcher/api.py:195]   rdzv_backend     : c10d\n",
      "i.generate/0 I0722 02:48:46.689000 2043 torch/distributed/launcher/api.py:195]   rdzv_endpoint    : localhost:0\n",
      "i.generate/0 I0722 02:48:46.689000 2043 torch/distributed/launcher/api.py:195]   rdzv_configs     : {'timeout': 900}\n",
      "i.generate/0 I0722 02:48:46.689000 2043 torch/distributed/launcher/api.py:195]   max_restarts     : 0\n",
      "i.generate/0 I0722 02:48:46.689000 2043 torch/distributed/launcher/api.py:195]   monitor_interval : 0.1\n",
      "i.generate/0 I0722 02:48:46.689000 2043 torch/distributed/launcher/api.py:195]   log_dir          : /root/.nemo_run/experiments/nemo.collections.llm.api.generate/nemo.collections.llm.api.generate_1753152524/nemo.collections.llm.api.generate/nemo_run/nemo.collections.llm.api.generate-rkt9qbwdzq9npc/torchelastic/nemo.collections.llm.api.generate\n",
      "i.generate/0 I0722 02:48:46.689000 2043 torch/distributed/launcher/api.py:195]   metrics_cfg      : {}\n",
      "i.generate/0 I0722 02:48:46.689000 2043 torch/distributed/launcher/api.py:195] \n",
      "i.generate/0 I0722 02:48:46.694000 2043 torch/distributed/elastic/agent/server/api.py:860] [default] starting workers for entrypoint: python\n",
      "i.generate/0 I0722 02:48:46.694000 2043 torch/distributed/elastic/agent/server/api.py:677] [default] Rendezvous'ing worker group\n",
      "i.generate/0 I0722 02:48:46.837000 2043 torch/distributed/elastic/agent/server/api.py:525] [default] Rendezvous complete for workers. Result:\n",
      "i.generate/0 I0722 02:48:46.837000 2043 torch/distributed/elastic/agent/server/api.py:525]   restart_count=0\n",
      "i.generate/0 I0722 02:48:46.837000 2043 torch/distributed/elastic/agent/server/api.py:525]   master_addr=69765b3e9bd8\n",
      "i.generate/0 I0722 02:48:46.837000 2043 torch/distributed/elastic/agent/server/api.py:525]   master_port=42689\n",
      "i.generate/0 I0722 02:48:46.837000 2043 torch/distributed/elastic/agent/server/api.py:525]   group_rank=0\n",
      "i.generate/0 I0722 02:48:46.837000 2043 torch/distributed/elastic/agent/server/api.py:525]   group_world_size=1\n",
      "i.generate/0 I0722 02:48:46.837000 2043 torch/distributed/elastic/agent/server/api.py:525]   local_ranks=[0]\n",
      "i.generate/0 I0722 02:48:46.837000 2043 torch/distributed/elastic/agent/server/api.py:525]   role_ranks=[0]\n",
      "i.generate/0 I0722 02:48:46.837000 2043 torch/distributed/elastic/agent/server/api.py:525]   global_ranks=[0]\n",
      "i.generate/0 I0722 02:48:46.837000 2043 torch/distributed/elastic/agent/server/api.py:525]   role_world_sizes=[1]\n",
      "i.generate/0 I0722 02:48:46.837000 2043 torch/distributed/elastic/agent/server/api.py:525]   global_world_sizes=[1]\n",
      "i.generate/0 I0722 02:48:46.837000 2043 torch/distributed/elastic/agent/server/api.py:525] \n",
      "i.generate/0 I0722 02:48:46.838000 2043 torch/distributed/elastic/agent/server/api.py:685] [default] Starting worker group\n",
      "i.generate/0 I0722 02:48:46.839000 2043 torch/distributed/elastic/agent/server/local_elastic_agent.py:298] use_agent_store: True\n",
      "i.generate/0 I0722 02:48:46.840000 2043 torch/distributed/elastic/agent/server/local_elastic_agent.py:192] Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.\n",
      "i.generate/0 I0722 02:48:46.841000 2043 torch/distributed/elastic/agent/server/local_elastic_agent.py:236] Environment variable 'TORCHELASTIC_HEALTH_CHECK_PORT' not found. Do not start health check.\n",
      "i.generate/0 [default0]:[NeMo W 2025-07-22 02:49:01 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "i.generate/0 [default0]:      warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n",
      "i.generate/0 [default0]:    \n",
      "i.generate/0 [default0]:GPU available: True (cuda), used: True\n",
      "i.generate/0 [default0]:TPU available: False, using: 0 TPU cores\n",
      "i.generate/0 [default0]:HPU available: False, using: 0 HPUs\n",
      "i.generate/0 [default0]:[NeMo I 2025-07-22 02:49:04 nemo_logging:393] Rank 0 has data parallel group : [0]\n",
      "i.generate/0 [default0]:[NeMo I 2025-07-22 02:49:04 nemo_logging:393] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "i.generate/0 [default0]:[NeMo I 2025-07-22 02:49:04 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0]]\n",
      "i.generate/0 [default0]:[NeMo I 2025-07-22 02:49:04 nemo_logging:393] Ranks 0 has data parallel rank: 0\n",
      "i.generate/0 [default0]:[NeMo I 2025-07-22 02:49:04 nemo_logging:393] Rank 0 has context parallel group: [0]\n",
      "i.generate/0 [default0]:[NeMo I 2025-07-22 02:49:04 nemo_logging:393] All context parallel group ranks: [[0]]\n",
      "i.generate/0 [default0]:[NeMo I 2025-07-22 02:49:04 nemo_logging:393] Ranks 0 has context parallel rank: 0\n",
      "i.generate/0 [default0]:[NeMo I 2025-07-22 02:49:04 nemo_logging:393] Rank 0 has model parallel group: [0]\n",
      "i.generate/0 [default0]:[NeMo I 2025-07-22 02:49:04 nemo_logging:393] All model parallel group ranks: [[0]]\n",
      "i.generate/0 [default0]:[NeMo I 2025-07-22 02:49:04 nemo_logging:393] Rank 0 has tensor model parallel group: [0]\n",
      "i.generate/0 [default0]:[NeMo I 2025-07-22 02:49:04 nemo_logging:393] All tensor model parallel group ranks: [[0]]\n",
      "i.generate/0 [default0]:[NeMo I 2025-07-22 02:49:04 nemo_logging:393] Rank 0 has tensor model parallel rank: 0\n",
      "i.generate/0 [default0]:[NeMo I 2025-07-22 02:49:04 nemo_logging:393] Rank 0 has pipeline model parallel group: [0]\n",
      "i.generate/0 [default0]:[NeMo I 2025-07-22 02:49:04 nemo_logging:393] Rank 0 has embedding group: [0]\n",
      "i.generate/0 [default0]:[NeMo I 2025-07-22 02:49:04 nemo_logging:393] All pipeline model parallel group ranks: [[0]]\n",
      "i.generate/0 [default0]:[NeMo I 2025-07-22 02:49:04 nemo_logging:393] Rank 0 has pipeline model parallel rank 0\n",
      "i.generate/0 [default0]:[NeMo I 2025-07-22 02:49:04 nemo_logging:393] All embedding group ranks: [[0]]\n",
      "i.generate/0 [default0]:[NeMo I 2025-07-22 02:49:04 nemo_logging:393] Rank 0 has embedding rank: 0\n",
      "i.generate/0 [default0]:[NeMo I 2025-07-22 02:49:04 nemo_logging:393] Use preset vocab_size: 151936, original vocab_size: 151665, dummy tokens: 271.\n",
      "i.generate/0 [default0]:----------------------------------------------------------------------------------------------------\n",
      "i.generate/0 [default0]:distributed_backend=nccl\n",
      "i.generate/0 [default0]:All distributed processes registered. Starting with 1 processes\n",
      "i.generate/0 [default0]:----------------------------------------------------------------------------------------------------\n",
      "i.generate/0 [default0]:\n",
      "i.generate/0 [default0]:[NeMo I 2025-07-22 02:49:04 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0 ,0): 1777088000\n",
      "i.generate/0 [default0]:[NeMo I 2025-07-22 02:49:04 nemo_logging:393] Doing selective restore from RestoreConfig(path='logs/qwen_sft/checkpoints/qwen_sft--reduced_train_loss=0.6889-epoch=0-consumed_samples=960.0-last', adapter_path=None, load_model_state=True, load_optim_state=False, load_artifacts=True)\n",
      "i.generate/0 [default0]:[NeMo I 2025-07-22 02:49:04 nemo_logging:393] Using <megatron.core.dist_checkpointing.strategies.fully_parallel.FullyParallelLoadStrategyWrapper object at 0x7ba5740def00> dist-ckpt load strategy.\n",
      "i.generate/0 [default0]:[NeMo I 2025-07-22 02:49:06 nemo_logging:393] Global Checkpoint Load : Rank : 0 : Start time : 1753152544.524s : Time spent in load_checkpoint: 2.209s\n",
      "i.generate/0 [default0]:[NeMo I 2025-07-22 02:49:06 nemo_logging:393] Restoring model weights from RestoreConfig(path='logs/qwen_sft/checkpoints/qwen_sft--reduced_train_loss=0.6889-epoch=0-consumed_samples=960.0-last', adapter_path=None, load_model_state=True, load_optim_state=False, load_artifacts=True)\n",
      "i.generate/0 [default0]:[NeMo I 2025-07-22 02:49:06 nemo_logging:393] Finished restoring from RestoreConfig(path='logs/qwen_sft/checkpoints/qwen_sft--reduced_train_loss=0.6889-epoch=0-consumed_samples=960.0-last', adapter_path=None, load_model_state=True, load_optim_state=False, load_artifacts=True), cleaning up.\n",
      "i.generate/0 [default0]:\n",
      "i.generate/0 [default0]:static requests:   0%|          | 0/2 [00:00<?, ?it/s]i.generate/0 [default0]:[NeMo I 2025-07-22 02:52:07 nemo_logging:393] Predictions written to qwen_after_sft_prediction.jsonl\n",
      "i.generate/0 [default0]:\n",
      "i.generate/0 [default0]:static requests: 100%|██████████| 2/2 [03:00<00:00, 90.25s/it]\n",
      "i.generate/0 [default0]:static requests: 100%|██████████| 2/2 [03:00<00:00, 90.25s/it]\n",
      "i.generate/0 I0722 02:52:12.441000 2043 torch/distributed/elastic/agent/server/api.py:879] [default] worker group successfully finished. Waiting 300 seconds for other agents to finish.\n",
      "i.generate/0 I0722 02:52:12.443000 2043 torch/distributed/elastic/agent/server/api.py:932] Local worker group finished (WorkerState.SUCCEEDED). Waiting 300 seconds for other agents to finish\n",
      "i.generate/0 I0722 02:52:12.445000 2043 torch/distributed/elastic/agent/server/api.py:946] Done waiting for other agents. Elapsed: 0.00040221214294433594 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job nemo.collections.llm.api.generate-rkt9qbwdzq9npc finished: SUCCEEDED\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># The experiment was run with the following tasks: ['nemo.collections.llm.api.generate']</span><span style=\"background-color: #272822\">                           </span>\n",
       "<span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># You can inspect and reconstruct this experiment at a later point in time using:</span><span style=\"background-color: #272822\">                                  </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> run</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">Experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">from_id(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"nemo.collections.llm.api.generate_1753152524\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">status() </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Gets the overall status</span><span style=\"background-color: #272822\">                                                                      </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">logs(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"nemo.collections.llm.api.generate\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">) </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Gets the log for the provided task</span><span style=\"background-color: #272822\">                          </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">cancel(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"nemo.collections.llm.api.generate\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">) </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Cancels the provided task if still running</span><span style=\"background-color: #272822\">                </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;149;144;119;48;2;39;40;34m# The experiment was run with the following tasks: ['nemo.collections.llm.api.generate']\u001b[0m\u001b[48;2;39;40;34m                           \u001b[0m\n",
       "\u001b[38;2;149;144;119;48;2;39;40;34m# You can inspect and reconstruct this experiment at a later point in time using:\u001b[0m\u001b[48;2;39;40;34m                                  \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mrun\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mExperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfrom_id\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mnemo.collections.llm.api.generate_1753152524\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mstatus\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Gets the overall status\u001b[0m\u001b[48;2;39;40;34m                                                                      \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlogs\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mnemo.collections.llm.api.generate\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Gets the log for the provided task\u001b[0m\u001b[48;2;39;40;34m                          \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcancel\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mnemo.collections.llm.api.generate\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Cancels the provided task if still running\u001b[0m\u001b[48;2;39;40;34m                \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># You can inspect this experiment at a later point in time using the CLI as well:</span><span style=\"background-color: #272822\">                                  </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">nemo experiment status nemo.collections.llm.api.generate_1753152524</span><span style=\"background-color: #272822\">                                                </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">nemo experiment logs nemo.collections.llm.api.generate_1753152524 </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">0</span><span style=\"background-color: #272822\">                                                </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">nemo experiment cancel nemo.collections.llm.api.generate_1753152524 </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">0</span><span style=\"background-color: #272822\">                                              </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;149;144;119;48;2;39;40;34m# You can inspect this experiment at a later point in time using the CLI as well:\u001b[0m\u001b[48;2;39;40;34m                                  \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mnemo\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mstatus\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnemo.collections.llm.api.generate_1753152524\u001b[0m\u001b[48;2;39;40;34m                                                \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mnemo\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlogs\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnemo.collections.llm.api.generate_1753152524\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m0\u001b[0m\u001b[48;2;39;40;34m                                                \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mnemo\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcancel\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnemo.collections.llm.api.generate_1753152524\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m0\u001b[0m\u001b[48;2;39;40;34m                                              \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from megatron.core.inference.common_inference_params import CommonInferenceParams\n",
    "\n",
    "sft_ckpt_path=str(next((d for d in Path(\"./logs/qwen_sft/checkpoints/\").iterdir() if d.is_dir() and d.name.endswith(\"-last\")), None))\n",
    "print(\"We will load SFT checkpoint from:\", sft_ckpt_path)\n",
    "\n",
    "\n",
    "def configure_inference():\n",
    "    return run.Partial(\n",
    "        llm.generate,\n",
    "        path=str(sft_ckpt_path),\n",
    "        trainer=trainer(),\n",
    "        prompts=prompts,\n",
    "        inference_params=CommonInferenceParams(num_tokens_to_generate=512, top_p=0.6),\n",
    "        output_path=\"qwen_after_sft_prediction.jsonl\",\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run.run(configure_inference(), executor=local_executor_torchrun())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Convert Fine-tuned Model to HF Format.\n",
    "\n",
    "After training, you can convert the output model to hf format using the `llm.export_ckpt` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will load SFT checkpoint from: logs/qwen_sft/checkpoints/qwen_sft--reduced_train_loss=0.6889-epoch=0-consumed_samples=960.0-last\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─ </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Entering Experiment nemo.collections.llm.api.export_ckpt with id: nemo.collections.llm.api.export_ckpt_1753154…</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ─</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─ \u001b[0m\u001b[1;35mEntering Experiment nemo.collections.llm.api.export_ckpt with id: nemo.collections.llm.api.export_ckpt_1753154…\u001b[0m\u001b[92m ─\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Log directory is: /root/.nemo_run/experiments/nemo.collections.llm.api.export_ckpt/nemo.collections.llm.api.export_ckpt_1753154969/nemo.collections.llm.api.export_ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[03:29:29] </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Launching job nemo.collections.llm.api.export_ckpt for experiment </span>                     <a href=\"file:///opt/Run/nemo_run/run/experiment.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">experiment.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/Run/nemo_run/run/experiment.py#744\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">744</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">nemo.collections.llm.api.export_ckpt</span>                                                   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                 </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[03:29:29]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;36mLaunching job nemo.collections.llm.api.export_ckpt for experiment \u001b[0m                     \u001b]8;id=691985;file:///opt/Run/nemo_run/run/experiment.py\u001b\\\u001b[2mexperiment.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=240916;file:///opt/Run/nemo_run/run/experiment.py#744\u001b\\\u001b[2m744\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m\u001b[1;36mnemo.collections.llm.api.export_ckpt\u001b[0m                                                   \u001b[2m                 \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Log directory is: /root/.nemo_run/experiments/nemo.collections.llm.api.export_ckpt/nemo.collections.llm.api.export_ckpt_1753154969/nemo.collections.llm.api.export_ckpt\n",
      "Launched app: local_persistent://nemo_run/nemo.collections.llm.api.export_ckpt-lq1g0w4ztp69vd\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">──────────────── </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Waiting for Experiment nemo.collections.llm.api.export_ckpt_1753154969 to finish</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ─────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m──────────────── \u001b[0m\u001b[1;35mWaiting for Experiment nemo.collections.llm.api.export_ckpt_1753154969 to finish\u001b[0m\u001b[92m ─────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Experiment Status for</span> <span style=\"color: #ffaf00; text-decoration-color: #ffaf00; font-weight: bold\">nemo.collections.llm.api.export_ckpt_1753154969</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mExperiment Status for\u001b[0m \u001b[1;38;5;214mnemo.collections.llm.api.export_ckpt_1753154969\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Task 0</span>: <span style=\"color: #ffaf00; text-decoration-color: #ffaf00; font-weight: bold\">nemo.collections.llm.api.export_ckpt</span>\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Status</span>: RUNNING\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Executor</span>: LocalExecutor\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Job id</span>: nemo.collections.llm.api.export_ckpt-lq1g0w4ztp69vd\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Local Directory</span>: /root/.nemo_run/experiments/nemo.collections.llm.api.export_ckpt/nemo.collections.llm.api.export_ckpt_1753154969/nemo.collections.llm.api.export_ckpt\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;32mTask 0\u001b[0m: \u001b[1;38;5;214mnemo.collections.llm.api.export_ckpt\u001b[0m\n",
       "- \u001b[1;32mStatus\u001b[0m: RUNNING\n",
       "- \u001b[1;32mExecutor\u001b[0m: LocalExecutor\n",
       "- \u001b[1;32mJob id\u001b[0m: nemo.collections.llm.api.export_ckpt-lq1g0w4ztp69vd\n",
       "- \u001b[1;32mLocal Directory\u001b[0m: /root/.nemo_run/experiments/nemo.collections.llm.api.export_ckpt/nemo.collections.llm.api.export_ckpt_1753154969/nemo.collections.llm.api.export_ckpt\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Waiting for job nemo.collections.llm.api.export_ckpt-lq1g0w4ztp69vd to finish [log=True]...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xport_ckpt/0 [NeMo W 2025-07-22 03:29:44 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "xport_ckpt/0       warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n",
      "xport_ckpt/0     \n",
      "xport_ckpt/0 [NeMo I 2025-07-22 03:29:46 nemo_logging:393] Rank 0 has data parallel group : [0]\n",
      "xport_ckpt/0 [NeMo I 2025-07-22 03:29:46 nemo_logging:393] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "xport_ckpt/0 [NeMo I 2025-07-22 03:29:46 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0]]\n",
      "xport_ckpt/0 [NeMo I 2025-07-22 03:29:46 nemo_logging:393] Ranks 0 has data parallel rank: 0\n",
      "xport_ckpt/0 [NeMo I 2025-07-22 03:29:46 nemo_logging:393] Rank 0 has context parallel group: [0]\n",
      "xport_ckpt/0 [NeMo I 2025-07-22 03:29:46 nemo_logging:393] All context parallel group ranks: [[0]]\n",
      "xport_ckpt/0 [NeMo I 2025-07-22 03:29:46 nemo_logging:393] Ranks 0 has context parallel rank: 0\n",
      "xport_ckpt/0 [NeMo I 2025-07-22 03:29:46 nemo_logging:393] Rank 0 has model parallel group: [0]\n",
      "xport_ckpt/0 [NeMo I 2025-07-22 03:29:46 nemo_logging:393] All model parallel group ranks: [[0]]\n",
      "xport_ckpt/0 [NeMo I 2025-07-22 03:29:46 nemo_logging:393] Rank 0 has tensor model parallel group: [0]\n",
      "xport_ckpt/0 [NeMo I 2025-07-22 03:29:46 nemo_logging:393] All tensor model parallel group ranks: [[0]]\n",
      "xport_ckpt/0 [NeMo I 2025-07-22 03:29:46 nemo_logging:393] Rank 0 has tensor model parallel rank: 0\n",
      "xport_ckpt/0 [NeMo I 2025-07-22 03:29:46 nemo_logging:393] Rank 0 has pipeline model parallel group: [0]\n",
      "xport_ckpt/0 [NeMo I 2025-07-22 03:29:46 nemo_logging:393] Rank 0 has embedding group: [0]\n",
      "xport_ckpt/0 [NeMo I 2025-07-22 03:29:46 nemo_logging:393] All pipeline model parallel group ranks: [[0]]\n",
      "xport_ckpt/0 [NeMo I 2025-07-22 03:29:46 nemo_logging:393] Rank 0 has pipeline model parallel rank 0\n",
      "xport_ckpt/0 [NeMo I 2025-07-22 03:29:46 nemo_logging:393] All embedding group ranks: [[0]]\n",
      "xport_ckpt/0 [NeMo I 2025-07-22 03:29:46 nemo_logging:393] Rank 0 has embedding rank: 0\n",
      "xport_ckpt/0 [NeMo I 2025-07-22 03:29:46 nemo_logging:393] Use preset vocab_size: 151936, original vocab_size: 151665, dummy tokens: 271.\n",
      "xport_ckpt/0 GPU available: True (cuda), used: False\n",
      "xport_ckpt/0 TPU available: False, using: 0 TPU cores\n",
      "xport_ckpt/0 HPU available: False, using: 0 HPUs\n",
      "xport_ckpt/0 [NeMo W 2025-07-22 03:29:46 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "xport_ckpt/0     \n",
      "xport_ckpt/0 ----------------------------------------------------------------------------------------------------\n",
      "xport_ckpt/0 distributed_backend=gloo\n",
      "xport_ckpt/0 All distributed processes registered. Starting with 1 processes\n",
      "xport_ckpt/0 ----------------------------------------------------------------------------------------------------\n",
      "xport_ckpt/0 \n",
      "xport_ckpt/0 [NeMo I 2025-07-22 03:29:48 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0 ,0): 1777088000\n",
      "xport_ckpt/0 [NeMo I 2025-07-22 03:29:48 nemo_logging:393] Using <megatron.core.dist_checkpointing.strategies.fully_parallel.FullyParallelLoadStrategyWrapper object at 0x7d2d65cb49e0> dist-ckpt load strategy.\n",
      "xport_ckpt/0 [NeMo W 2025-07-22 03:29:48 nemo_logging:405] Could not copy Trainer's 'max_steps' to LR scheduler's 'max_steps'. If you are not using an LR scheduler, this warning can safely be ignored.\n",
      "xport_ckpt/0 [NeMo I 2025-07-22 03:29:51 nemo_logging:393] Global Checkpoint Load : Rank : 0 : Start time : 1753154988.527s : Time spent in load_checkpoint: 3.310s\n",
      "xport_ckpt/0 GPU available: True (cuda), used: True\n",
      "xport_ckpt/0 TPU available: False, using: 0 TPU cores\n",
      "xport_ckpt/0 HPU available: False, using: 0 HPUs\n",
      "xport_ckpt/0 GPU available: True (cuda), used: True\n",
      "xport_ckpt/0 TPU available: False, using: 0 TPU cores\n",
      "xport_ckpt/0 HPU available: False, using: 0 HPUs\n",
      "xport_ckpt/0 Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "xport_ckpt/0 GPU available: True (cuda), used: True\n",
      "xport_ckpt/0 TPU available: False, using: 0 TPU cores\n",
      "xport_ckpt/0 HPU available: False, using: 0 HPUs\n",
      "xport_ckpt/0 \u001b[32m✓ Checkpoint exported to model\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job nemo.collections.llm.api.export_ckpt-lq1g0w4ztp69vd finished: SUCCEEDED\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># The experiment was run with the following tasks: ['nemo.collections.llm.api.export_ckpt']</span><span style=\"background-color: #272822\">                        </span>\n",
       "<span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># You can inspect and reconstruct this experiment at a later point in time using:</span><span style=\"background-color: #272822\">                                  </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> run</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">Experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">from_id(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"nemo.collections.llm.api.export_ckpt_1753154969\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                             </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">status() </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Gets the overall status</span><span style=\"background-color: #272822\">                                                                      </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">logs(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"nemo.collections.llm.api.export_ckpt\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">) </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Gets the log for the provided task</span><span style=\"background-color: #272822\">                       </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">cancel(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"nemo.collections.llm.api.export_ckpt\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">) </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Cancels the provided task if still running</span><span style=\"background-color: #272822\">             </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;149;144;119;48;2;39;40;34m# The experiment was run with the following tasks: ['nemo.collections.llm.api.export_ckpt']\u001b[0m\u001b[48;2;39;40;34m                        \u001b[0m\n",
       "\u001b[38;2;149;144;119;48;2;39;40;34m# You can inspect and reconstruct this experiment at a later point in time using:\u001b[0m\u001b[48;2;39;40;34m                                  \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mrun\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mExperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfrom_id\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mnemo.collections.llm.api.export_ckpt_1753154969\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                             \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mstatus\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Gets the overall status\u001b[0m\u001b[48;2;39;40;34m                                                                      \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlogs\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mnemo.collections.llm.api.export_ckpt\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Gets the log for the provided task\u001b[0m\u001b[48;2;39;40;34m                       \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcancel\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mnemo.collections.llm.api.export_ckpt\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Cancels the provided task if still running\u001b[0m\u001b[48;2;39;40;34m             \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># You can inspect this experiment at a later point in time using the CLI as well:</span><span style=\"background-color: #272822\">                                  </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">nemo experiment status nemo.collections.llm.api.export_ckpt_1753154969</span><span style=\"background-color: #272822\">                                             </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">nemo experiment logs nemo.collections.llm.api.export_ckpt_1753154969 </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">0</span><span style=\"background-color: #272822\">                                             </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">nemo experiment cancel nemo.collections.llm.api.export_ckpt_1753154969 </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">0</span><span style=\"background-color: #272822\">                                           </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;149;144;119;48;2;39;40;34m# You can inspect this experiment at a later point in time using the CLI as well:\u001b[0m\u001b[48;2;39;40;34m                                  \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mnemo\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mstatus\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnemo.collections.llm.api.export_ckpt_1753154969\u001b[0m\u001b[48;2;39;40;34m                                             \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mnemo\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlogs\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnemo.collections.llm.api.export_ckpt_1753154969\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m0\u001b[0m\u001b[48;2;39;40;34m                                             \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mnemo\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcancel\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnemo.collections.llm.api.export_ckpt_1753154969\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m0\u001b[0m\u001b[48;2;39;40;34m                                           \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "sft_ckpt_path=str(next((d for d in Path(\"./logs/qwen_sft/checkpoints/\").iterdir() if d.is_dir() and d.name.endswith(\"-last\")), None))\n",
    "\n",
    "print(\"We will load SFT checkpoint from:\", sft_ckpt_path)\n",
    "\n",
    "# llm.export_ckpt is the nemo2 API for exporting a NeMo checkpoint to Hugging Face format\n",
    "# example python usage:\n",
    "# llm.export_ckpt(path=\"/path/to/model.nemo\", target=\"hf\", output_path=\"/path/to/save\")\n",
    "def configure_checkpoint_conversion():\n",
    "    return run.Partial(\n",
    "        llm.export_ckpt,\n",
    "        path=sft_ckpt_path,\n",
    "        target=\"hf\",\n",
    "        output_path=\"./model\",\n",
    "    )\n",
    "\n",
    "# configure your function\n",
    "export_ckpt = configure_checkpoint_conversion()\n",
    "# define your executor\n",
    "local_executor = run.LocalExecutor()\n",
    "\n",
    "# run your experiment\n",
    "run.run(export_ckpt, executor=local_executor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
