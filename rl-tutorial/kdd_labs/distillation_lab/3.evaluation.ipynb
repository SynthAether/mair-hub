{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation with Math500 Benchmark\n",
    "\n",
    "In the field of LLMs, reasoning models leverage deep thinking capabilities to significantly enhance model performance across complex scenarios. According to the [DeepSeek-R1](https://arxiv.org/abs/2501.12948) paper, the reasoning pattern of larger models can be distilled into smaller models. Specifically, we can distill long-chain-of-thought (long-CoT) data that includes reasoning processes from DeepSeek-R1 and directly fine-tune open-source models like Qwen and Llama. This straightforward distillation method significantly enhances the reasoning abilities of smaller models.\n",
    "\n",
    "To demonstrate the complete distillation process, we have prepared three notebooks that cover how to distill reasoning data from DeepSeek-R1 using the NIM API, how to train models using the distilled data, and how to evaluate the model.\n",
    "\n",
    "- [1.generate_reasoning_data.ipynb](./1.generate_reasoning_data.ipynb) demonstrates how to distill reasoning data from DeepSeek-R1 using the NIM API. \n",
    "- [2.qwen2_distill_nemo.ipynb](./2.qwen2_distill_nemo.ipynb)  shows how to train open-source models using the distilled data.\n",
    "- [3.evaluation.ipynb](./3.evaluation.ipynb) (⭐) shows how the evaluate the model.\n",
    "\n",
    "\n",
    "\n",
    "This notebook is part 3 of the series. After completing the model training and distillation process, it's essential to evaluate the model's performance on standardized benchmarks to assess its reasoning capabilities. This notebook demonstrates the evaluation workflow using the [Math500](https://huggingface.co/datasets/HuggingFaceH4/MATH-500) dataset as an example.\n",
    "\n",
    "The evaluation process consists of three main steps:\n",
    "\n",
    "1. **Start a vLLM Inference Server** - Deploy the trained model for inference\n",
    "2. **Generate Responses** - Use the model to generate answers for benchmark questions\n",
    "3. **Calculate Evaluation Metrics** - Assess the model's performance using appropriate metrics\n",
    "\n",
    "This tutorial demonstrates how to evaluate reasoning models trained with distilled data from DeepSeek-R1, focusing on mathematical problem-solving capabilities.\n",
    "\n",
    "Prerequisites:\n",
    "- A trained model checkpoint (from the previous distillation and training steps)\n",
    "- vLLM installed in your environment\n",
    "- Access to the Math500 benchmark dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets openai vllm requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Start vLLM Inference Server\n",
    "\n",
    "Since vLLM server needs to run as a separate process, we cannot start it directly within this Jupyter notebook. Please follow these instructions to start the vLLM inference server in a separate terminal:\n",
    "\n",
    "\n",
    "1. **Open a new terminal window**\n",
    "\n",
    "2. **Navigate to your model directory** (replace with your actual model path):\n",
    "   ```bash\n",
    "   cd /path/to/your/trained/model\n",
    "   ```\n",
    "\n",
    "3. **Start the vLLM server**:\n",
    "   ```bash\n",
    "    python -m vllm.entrypoints.openai.api_server \\\n",
    "       --model ./model \\\n",
    "       --host 0.0.0.0 \\\n",
    "       --port 8000 \\\n",
    "       --tensor-parallel-size 1 \\\n",
    "       --gpu-memory-utilization 0.9 \\\n",
    "       --served-model-name qwen_distill\n",
    "   ```\n",
    "\n",
    "### Parameters Explanation:\n",
    "- `--model`: Path to your trained model\n",
    "- `--host 0.0.0.0`: Allow connections from any IP\n",
    "- `--port 8000`: Port number for the API server\n",
    "- `--tensor-parallel-size`: Number of GPUs for tensor parallelism\n",
    "- `--gpu-memory-utilization`: Fraction of GPU memory to use\n",
    "- `--served-model-name`: Rename the model for API calls (here set to `qwen_distill`)\n",
    "\n",
    "### Verify Server is Running\n",
    "\n",
    "Once the server starts, you should see output similar to:\n",
    "```\n",
    "INFO:     Started server process\n",
    "INFO:     Waiting for application startup.\n",
    "INFO:     Application startup complete.\n",
    "INFO:     Uvicorn running on http://0.0.0.0:8000\n",
    "```\n",
    "\n",
    "You can verify the server is working by visiting `http://localhost:8000/docs` in your browser or running the test cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ vLLM server is running successfully!\n"
     ]
    }
   ],
   "source": [
    "# Test if vLLM server is running\n",
    "import requests\n",
    "try:\n",
    "    response = requests.get(\"http://localhost:8000/health\")\n",
    "    if response.status_code == 200:\n",
    "        print(\"✅ vLLM server is running successfully!\")\n",
    "    else:\n",
    "        print(f\"❌ Server responded with status code: {response.status_code}\")\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"❌ Cannot connect to vLLM server. Please make sure it's running on localhost:8000\")\n",
    "    print(\"Follow the instructions above to start the server.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** \n",
    "To quickly verify the effect, we set the model's maximum generation length to `4096`.    \n",
    "If you want to verify the complete model effect, please follow the steps below:  \n",
    "1. Modify the model's `max_position_embeddings` in `model/config.json` from `4096` to `32768`, and restart the vllm server.\n",
    "2. Modify the `MAX_GEN_TOKENS` parameter below to `8192`.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_GEN_TOKENS=3500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Math500 Dataset and Generate Responses\n",
    "\n",
    "Now we'll load the [Math500](https://huggingface.co/datasets/HuggingFaceH4/MATH-500) benchmark dataset, sample 10 questions, and use our trained model to generate responses. The Math500 dataset contains mathematical problems that test various reasoning capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"HuggingFaceH4/MATH-500\", split=\"test\")\n",
    "\n",
    "# Sample 10 random questions for evaluation\n",
    "random.seed(42)  # For reproducibility\n",
    "sample_size = 10\n",
    "sample_indices = random.sample(range(len(dataset)), sample_size)\n",
    "sample_problems = dataset.select(sample_indices)\n",
    "\n",
    "print(f\"Sampled {len(sample_problems)} problems for evaluation\")\n",
    "\n",
    "# Display the first few problems\n",
    "for i, problem in enumerate(sample_problems.select(range(3))):\n",
    "    print(f\"\\n===== Problem {i+1} =====\")\n",
    "    if \"problem\" in problem:\n",
    "        print(problem[\"problem\"])\n",
    "    else:\n",
    "        print(problem)\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client for vLLM server\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    "    api_key=\"not-needed-for-local\"  # vLLM doesn't require API key for local deployment\n",
    ")\n",
    "\n",
    "# Generate responses for the sample problems\n",
    "def generate_response(problem_text, model_name, max_tokens=3500, temperature=0.6):\n",
    "    \"\"\"\n",
    "    Generate a response for a given math problem using the trained model\n",
    "    \"\"\"\n",
    "    # Use a prompt similar to the training format\n",
    "    prompt = f\"{problem_text} Please reason step by step, and put your final answer within \\\\boxed{{}}.\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            timeout=60\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response: {e}\")\n",
    "        return None\n",
    "\n",
    "# Store results\n",
    "evaluation_results = []\n",
    "model_name = \"qwen_distill\"  # Update this to match your model\n",
    "\n",
    "print(\"Generating responses for sample problems...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, problem in enumerate(sample_problems):\n",
    "    print(f\"\\nProcessing problem {i+1}/{len(sample_problems)}...\")\n",
    "    \n",
    "    if \"problem\" in problem:\n",
    "        problem_text = problem[\"problem\"]\n",
    "        ground_truth = problem.get(\"answer\", \"N/A\")\n",
    "    else:\n",
    "        problem_text = str(problem)\n",
    "        ground_truth = \"N/A\"\n",
    "    \n",
    "    # Generate response\n",
    "    model_response = generate_response(problem_text, model_name, MAX_GEN_TOKENS)\n",
    "    \n",
    "    if model_response:\n",
    "        result = {\n",
    "            \"problem_id\": i,\n",
    "            \"problem\": problem_text,\n",
    "            \"ground_truth\": ground_truth,\n",
    "            \"model_response\": model_response\n",
    "        }\n",
    "        evaluation_results.append(result)\n",
    "        \n",
    "        print(f\"✅ Generated response for problem {i+1}\")\n",
    "        print(f\"Problem: {problem_text[:100]}...\")\n",
    "        print(f\"Response length: {len(model_response)} characters\")\n",
    "    else:\n",
    "        print(f\"❌ Failed to generate response for problem {i+1}\")\n",
    "    \n",
    "    # Add a small delay to avoid overwhelming the server\n",
    "    time.sleep(1)\n",
    "\n",
    "print(f\"\\n🎉 Completed! Generated responses for {len(evaluation_results)} problems.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Evaluation Results:\n",
      "================================================================================\n",
      "\n",
      "--- Problem 1 ---\n",
      "Question: Let $P(x)$ be a monic polynomial of degree 3.  Suppose that $P(x)$ has remainder $R(x)$ when it is divided by $(x - 1)(x - 4),$ and remainder $2R(x)$ when it is divided by $(x - 2)(x - 3).$  Given tha...\n",
      "\n",
      "Model Response: Given that \\( P(x) \\) is a monic polynomial of degree 3, we can express it in the form \\( P(x) = x^3 + ax^2 + bx + c \\). The polynomial \\( P(x) \\) has a remainder \\( R(x) \\) when divided by \\( (x - 1)...\n",
      "\n",
      "Ground Truth: 15\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Problem 2 ---\n",
      "Question: Riproarin' Ringo was roping a recalcitrant dogie. Ringo decided to give the dogie a reprieve by calculating \\[|(1-i)^8|\\]before riding after the dogie. What answer should Ringo have found?...\n",
      "\n",
      "Model Response: Okay, let's see. The problem is asking for the absolute value of (1 - i)^8. Hmm, complex numbers here. Alright, let me recall how to handle complex exponents and absolute values.\n",
      "\n",
      "First, remember that...\n",
      "\n",
      "Ground Truth: 16\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Problem 3 ---\n",
      "Question: The proper divisors of 12 are 1, 2, 3, 4 and 6. A proper divisor of an integer $N$ is a positive divisor of $N$ that is less than $N$. What is the sum of the proper divisors of the sum of the proper d...\n",
      "\n",
      "Model Response: Okay, let's see. I need to find the sum of the proper divisors of 284, then find the proper divisors of that sum, and finally add those together. Hmm, right. Let me break this down step by step.\n",
      "\n",
      "Firs...\n",
      "\n",
      "Ground Truth: 284\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Display sample results\n",
    "print(\"Sample Evaluation Results:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, result in enumerate(evaluation_results[:3]):  # Show first 3 results\n",
    "    print(f\"\\n--- Problem {i+1} ---\")\n",
    "    print(f\"Question: {result['problem'][:200]}...\")\n",
    "    print(f\"\\nModel Response: {result['model_response'][:200]}...\")\n",
    "    print(f\"\\nGround Truth: {result['ground_truth']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to file for further analysis\n",
    "output_file = \"math500_evaluation_results.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(evaluation_results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"✅ Evaluation results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Calculate Evaluation Metrics with Math Verification\n",
    "\n",
    "In this step, we'll analyze the evaluation results by:\n",
    "\n",
    "1. **Installing and importing math_verify** - A library for mathematical equivalence checking\n",
    "2. **Extracting predicted answers** - Parse answers from model responses within `\\boxed{}` notation\n",
    "3. **Mathematical equivalence checking** - Compare predicted answers with ground truth using math_verify\n",
    "4. **Computing accuracy metrics** - Calculate overall accuracy and detailed statistics\n",
    "\n",
    "The math_verify library helps ensure that mathematically equivalent answers (like \"1/2\" and \"0.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install math_verify library for mathematical equivalence checking\n",
    "%pip install math_verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from math_verify import parse, verify\n",
    "\n",
    "def extract_boxed_answer(text: str) -> str:\n",
    "    \"\"\"Extract the content from the last \\\\boxed{} notation in the text.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Find all \\\\boxed{...} patterns\n",
    "    pattern = r'\\\\boxed\\{([^{}]*(?:\\{[^{}]*\\}[^{}]*)*)\\}'\n",
    "    matches = re.findall(pattern, text)\n",
    "    \n",
    "    # Return the last match, or empty string if no match found\n",
    "    return matches[-1] if matches else \"\"\n",
    "\n",
    "def check_answer_equivalence(predicted: str, ground_truth: str) -> bool:\n",
    "    \"\"\"Check if predicted answer is mathematically equivalent to ground truth using math_verify.\"\"\"\n",
    "    try:\n",
    "        parsed_pred = parse(predicted)\n",
    "        parsed_truth = parse(ground_truth)\n",
    "        return verify(parsed_pred, parsed_truth)\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 evaluation results\n",
      "==================================================\n",
      "Problem 1: ❌\n",
      "  Predicted: \\dfrac{35}{2}\n",
      "  Ground Truth: 15\n",
      "Problem 2: ❌\n",
      "  Predicted: \n",
      "  Ground Truth: 16\n",
      "Problem 3: ❌\n",
      "  Predicted: \n",
      "  Ground Truth: 284\n",
      "Problem 4: ✅\n",
      "  Predicted: 12\n",
      "  Ground Truth: 12\n",
      "Problem 5: ✅\n",
      "  Predicted: 13\n",
      "  Ground Truth: 13\n",
      "Problem 6: ✅\n",
      "  Predicted: -3\n",
      "  Ground Truth: -3\n",
      "Problem 7: ✅\n",
      "  Predicted: -5\n",
      "  Ground Truth: -5\n",
      "Problem 8: ❌\n",
      "  Predicted: 101561410_8\n",
      "  Ground Truth: 2516_8\n",
      "Problem 9: ❌\n",
      "  Predicted: \n",
      "  Ground Truth: 6\n",
      "Problem 10: ✅\n",
      "  Predicted: 12\n",
      "  Ground Truth: 12\n",
      "==================================================\n",
      "Final Results:\n",
      "Correct answers: 5/10\n",
      "Accuracy: 50.00%\n"
     ]
    }
   ],
   "source": [
    "# Load evaluation results\n",
    "with open(\"math500_evaluation_results.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(results)} evaluation results\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Evaluate each result\n",
    "correct_count = 0\n",
    "total_count = len(results)\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    predicted_answer = extract_boxed_answer(result['model_response'])\n",
    "    ground_truth = result['ground_truth']\n",
    "    \n",
    "    is_correct = check_answer_equivalence(predicted_answer, ground_truth)\n",
    "    \n",
    "    print(f\"Problem {i+1}: {'✅' if is_correct else '❌'}\")\n",
    "    print(f\"  Predicted: {predicted_answer}\")\n",
    "    print(f\"  Ground Truth: {ground_truth}\")\n",
    "    \n",
    "    if is_correct:\n",
    "        correct_count += 1\n",
    "\n",
    "# Calculate and display accuracy\n",
    "accuracy = correct_count / total_count * 100\n",
    "print(\"=\" * 50)\n",
    "print(f\"Final Results:\")\n",
    "print(f\"Correct answers: {correct_count}/{total_count}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
